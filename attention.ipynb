{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import os\n",
    "\n",
    "#指定第一块GPU可用 \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "config = tf.ConfigProto() \n",
    "#不全部占满显存, 按需分配\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Position_Embedding(Layer):\n",
    "    \n",
    "    def __init__(self, size=None, mode='sum', **kwargs):\n",
    "        self.size = size #必须为偶数\n",
    "        self.mode = mode\n",
    "        super(Position_Embedding, self).__init__(**kwargs)\n",
    "        \n",
    "    def call(self, x):\n",
    "        if (self.size == None) or (self.mode == 'sum'):\n",
    "            self.size = int(x.shape[-1])\n",
    "        batch_size,seq_len = K.shape(x)[0],K.shape(x)[1]\n",
    "        position_j = 1. / K.pow(10000., \\\n",
    "                                 2 * K.arange(self.size / 2, dtype='float32' \\\n",
    "                               ) / self.size)\n",
    "        position_j = K.expand_dims(position_j, 0)\n",
    "        position_i = K.cumsum(K.ones_like(x[:,:,0]), 1)-1 #K.arange不支持变长，只好用这种方法生成\n",
    "        position_i = K.expand_dims(position_i, 2)\n",
    "        position_ij = K.dot(position_i, position_j)\n",
    "        position_ij = K.concatenate([K.cos(position_ij), K.sin(position_ij)], 2)\n",
    "        if self.mode == 'sum':\n",
    "            return position_ij + x\n",
    "        elif self.mode == 'concat':\n",
    "            return K.concatenate([position_ij, x], 2)\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.mode == 'sum':\n",
    "            return input_shape\n",
    "        elif self.mode == 'concat':\n",
    "            return (input_shape[0], input_shape[1], input_shape[2]+self.size)\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "\n",
    "    def __init__(self, nb_head, size_per_head, mask_right=False, **kwargs):\n",
    "        self.nb_head = nb_head\n",
    "        self.size_per_head = size_per_head\n",
    "        self.output_dim = nb_head*size_per_head\n",
    "        self.mask_right = mask_right\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.WQ = self.add_weight(name='WQ', \n",
    "                                  shape=(input_shape[0][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WK = self.add_weight(name='WK', \n",
    "                                  shape=(input_shape[1][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WV = self.add_weight(name='WV', \n",
    "                                  shape=(input_shape[2][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "        \n",
    "    def Mask(self, inputs, seq_len, mode='mul'):\n",
    "        if seq_len == None:\n",
    "            return inputs\n",
    "        else:\n",
    "            mask = K.one_hot(seq_len[:,0], K.shape(inputs)[1])\n",
    "            mask = 1 - K.cumsum(mask, 1)\n",
    "            for _ in range(len(inputs.shape)-2):\n",
    "                mask = K.expand_dims(mask, 2)\n",
    "            if mode == 'mul':\n",
    "                return inputs * mask\n",
    "            if mode == 'add':\n",
    "                return inputs - (1 - mask) * 1e12\n",
    "                \n",
    "    def call(self, x):\n",
    "        #如果只传入Q_seq,K_seq,V_seq，那么就不做Mask\n",
    "        #如果同时传入Q_seq,K_seq,V_seq,Q_len,V_len，那么对多余部分做Mask\n",
    "        if len(x) == 3:\n",
    "            Q_seq,K_seq,V_seq = x\n",
    "            Q_len,V_len = None,None\n",
    "        elif len(x) == 5:\n",
    "            Q_seq,K_seq,V_seq,Q_len,V_len = x\n",
    "        #对Q、K、V做线性变换\n",
    "        Q_seq = K.dot(Q_seq, self.WQ)\n",
    "        Q_seq = K.reshape(Q_seq, (-1, K.shape(Q_seq)[1], self.nb_head, self.size_per_head))\n",
    "        Q_seq = K.permute_dimensions(Q_seq, (0,2,1,3))\n",
    "        K_seq = K.dot(K_seq, self.WK)\n",
    "        K_seq = K.reshape(K_seq, (-1, K.shape(K_seq)[1], self.nb_head, self.size_per_head))\n",
    "        K_seq = K.permute_dimensions(K_seq, (0,2,1,3))\n",
    "        V_seq = K.dot(V_seq, self.WV)\n",
    "        V_seq = K.reshape(V_seq, (-1, K.shape(V_seq)[1], self.nb_head, self.size_per_head))\n",
    "        V_seq = K.permute_dimensions(V_seq, (0,2,1,3))\n",
    "        #计算内积，然后mask，然后softmax\n",
    "        A = K.batch_dot(Q_seq, K_seq, axes=[3,3]) / self.size_per_head**0.5\n",
    "        A = K.permute_dimensions(A, (0,3,2,1))\n",
    "        A = self.Mask(A, V_len, 'add')\n",
    "        A = K.permute_dimensions(A, (0,3,2,1)) \n",
    "        if self.mask_right:\n",
    "            ones = K.ones_like(A[:1, :1])\n",
    "            mask = (ones - K.tf.matrix_band_part(ones, -1, 0)) * 1e12\n",
    "            A = A - mask\n",
    "        A = K.softmax(A)\n",
    "        #输出并mask\n",
    "        O_seq = K.batch_dot(A, V_seq, axes=[3,2])\n",
    "        O_seq = K.permute_dimensions(O_seq, (0,2,1,3))\n",
    "        O_seq = K.reshape(O_seq, (-1, K.shape(O_seq)[1], self.output_dim))\n",
    "        O_seq = self.Mask(O_seq, Q_len, 'mul')\n",
    "        return O_seq\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][1], self.output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "def attention_model():\n",
    "    S_inputs = Input(shape=(None,), dtype='int32')\n",
    "    embeddings = Embedding(max_features, 128)(S_inputs)\n",
    "    embeddings = Position_Embedding()(embeddings) # 增加Position_Embedding能轻微提高准确率\n",
    "    O_seq = Attention(8,16)([embeddings,embeddings,embeddings])\n",
    "    O_seq = GlobalAveragePooling1D()(O_seq)\n",
    "    O_seq = Dropout(0.5)(O_seq)\n",
    "    outputs = Dense(1, activation='sigmoid')(O_seq)\n",
    "\n",
    "    model = Model(inputs=S_inputs, outputs=outputs)\n",
    "    # try using different optimizers and different optimizer configs\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AA = 'GAVLIFWYDNEKQMSTCPHR'\n",
    "def pep(path, seq_len):\n",
    "    seqs = open(path).readlines()\n",
    "    cut = (len(seqs[0].split()[0]) - 1 - seq_len) // 2\n",
    "    X = [[AA.index(res.upper()) if res.upper() in AA else 0\n",
    "          for res in (seq.split()[0][cut:-cut] if cut != 0 else seq.split()[0])]\n",
    "        for seq in seqs if seq.strip() != '']\n",
    "    y = [int(seq.split()[-1]) for seq in seqs if seq.strip() != '']\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "\n",
    "S_inputs = Input(shape=(None,), dtype='int32')\n",
    "embeddings = Embedding(max_features, 128)(S_inputs)\n",
    "# embeddings = Position_Embedding()(embeddings) # 增加Position_Embedding能轻微提高准确率\n",
    "O_seq = Attention(8,16)([embeddings,embeddings,embeddings])\n",
    "O_seq = GlobalAveragePooling1D()(O_seq)\n",
    "O_seq = Dropout(0.5)(O_seq)\n",
    "outputs = Dense(1, activation='sigmoid')(O_seq)\n",
    "\n",
    "model = Model(inputs=S_inputs, outputs=outputs)\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=5,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "\n",
    "S_inputs = Input(shape=(None,), dtype='int32')\n",
    "embeddings = Embedding(max_features, 128)(S_inputs)\n",
    "# embeddings = Position_Embedding()(embeddings) # 增加Position_Embedding能轻微提高准确率\n",
    "O_seq = Attention(8,16)([embeddings,embeddings,embeddings])\n",
    "O_seq = GlobalAveragePooling1D()(O_seq)\n",
    "O_seq = Dropout(0.5)(O_seq)\n",
    "outputs = Dense(1, activation='sigmoid')(O_seq)\n",
    "\n",
    "model = Model(inputs=S_inputs, outputs=outputs)\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=5,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,re,time,math,csv\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import cluster\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "from keras import optimizers\n",
    "from keras import initializers, regularizers, constraints, callbacks\n",
    "\n",
    "\n",
    "import pickle\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "import matplotlib.mlab as mlab\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from scipy import interp\n",
    "from sklearn import metrics\n",
    "from keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer\n",
    "\n",
    "class Position_Embedding(Layer):\n",
    "    \n",
    "    def __init__(self, size=None, mode='sum', **kwargs):\n",
    "        self.size = size #必须为偶数\n",
    "        self.mode = mode\n",
    "        super(Position_Embedding, self).__init__(**kwargs)\n",
    "        \n",
    "    def call(self, x):\n",
    "        if (self.size == None) or (self.mode == 'sum'):\n",
    "            self.size = int(x.shape[-1])\n",
    "        batch_size,seq_len = K.shape(x)[0],K.shape(x)[1]\n",
    "        position_j = 1. / K.pow(10000., \\\n",
    "                                 2 * K.arange(self.size / 2, dtype='float32' \\\n",
    "                               ) / self.size)\n",
    "        position_j = K.expand_dims(position_j, 0)\n",
    "        position_i = K.cumsum(K.ones_like(x[:,:,0]), 1)-1 #K.arange不支持变长，只好用这种方法生成\n",
    "        position_i = K.expand_dims(position_i, 2)\n",
    "        position_ij = K.dot(position_i, position_j)\n",
    "        position_ij = K.concatenate([K.cos(position_ij), K.sin(position_ij)], 2)\n",
    "        if self.mode == 'sum':\n",
    "            return position_ij + x\n",
    "        elif self.mode == 'concat':\n",
    "            return K.concatenate([position_ij, x], 2)\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.mode == 'sum':\n",
    "            return input_shape\n",
    "        elif self.mode == 'concat':\n",
    "            return (input_shape[0], input_shape[1], input_shape[2]+self.size)\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "\n",
    "    def __init__(self, nb_head, size_per_head, mask_right=False, **kwargs):\n",
    "        self.nb_head = nb_head\n",
    "        self.size_per_head = size_per_head\n",
    "        self.output_dim = nb_head*size_per_head\n",
    "        self.mask_right = mask_right\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.WQ = self.add_weight(name='WQ', \n",
    "                                  shape=(input_shape[0][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WK = self.add_weight(name='WK', \n",
    "                                  shape=(input_shape[1][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WV = self.add_weight(name='WV', \n",
    "                                  shape=(input_shape[2][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "        \n",
    "    def Mask(self, inputs, seq_len, mode='mul'):\n",
    "        if seq_len == None:\n",
    "            return inputs\n",
    "        else:\n",
    "            mask = K.one_hot(seq_len[:,0], K.shape(inputs)[1])\n",
    "            mask = 1 - K.cumsum(mask, 1)\n",
    "            for _ in range(len(inputs.shape)-2):\n",
    "                mask = K.expand_dims(mask, 2)\n",
    "            if mode == 'mul':\n",
    "                return inputs * mask\n",
    "            if mode == 'add':\n",
    "                return inputs - (1 - mask) * 1e12\n",
    "                \n",
    "    def call(self, x):\n",
    "        #如果只传入Q_seq,K_seq,V_seq，那么就不做Mask\n",
    "        #如果同时传入Q_seq,K_seq,V_seq,Q_len,V_len，那么对多余部分做Mask\n",
    "        if len(x) == 3:\n",
    "            Q_seq,K_seq,V_seq = x\n",
    "            Q_len,V_len = None,None\n",
    "        elif len(x) == 5:\n",
    "            Q_seq,K_seq,V_seq,Q_len,V_len = x\n",
    "        #对Q、K、V做线性变换\n",
    "        Q_seq = K.dot(Q_seq, self.WQ)\n",
    "        Q_seq = K.reshape(Q_seq, (-1, K.shape(Q_seq)[1], self.nb_head, self.size_per_head))\n",
    "        Q_seq = K.permute_dimensions(Q_seq, (0,2,1,3))\n",
    "        K_seq = K.dot(K_seq, self.WK)\n",
    "        K_seq = K.reshape(K_seq, (-1, K.shape(K_seq)[1], self.nb_head, self.size_per_head))\n",
    "        K_seq = K.permute_dimensions(K_seq, (0,2,1,3))\n",
    "        V_seq = K.dot(V_seq, self.WV)\n",
    "        V_seq = K.reshape(V_seq, (-1, K.shape(V_seq)[1], self.nb_head, self.size_per_head))\n",
    "        V_seq = K.permute_dimensions(V_seq, (0,2,1,3))\n",
    "        #计算内积，然后mask，然后softmax\n",
    "        A = K.batch_dot(Q_seq, K_seq, axes=[3,3]) / self.size_per_head**0.5\n",
    "        A = K.permute_dimensions(A, (0,3,2,1))\n",
    "        A = self.Mask(A, V_len, 'add')\n",
    "        A = K.permute_dimensions(A, (0,3,2,1)) \n",
    "        if self.mask_right:\n",
    "            ones = K.ones_like(A[:1, :1])\n",
    "            mask = (ones - K.tf.matrix_band_part(ones, -1, 0)) * 1e12\n",
    "            A = A - mask\n",
    "        A = K.softmax(A)\n",
    "        #输出并mask\n",
    "        O_seq = K.batch_dot(A, V_seq, axes=[3,2])\n",
    "        O_seq = K.permute_dimensions(O_seq, (0,2,1,3))\n",
    "        O_seq = K.reshape(O_seq, (-1, K.shape(O_seq)[1], self.output_dim))\n",
    "        O_seq = self.Mask(O_seq, Q_len, 'mul')\n",
    "        return O_seq\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][1], self.output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model6(input_length=29,dropout=0.5):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(21, 5, input_length = input_length))\n",
    "    model.add(Conv1D(128, 8, activation='relu', padding='same'))\n",
    "    #model.add(MaxPooling1D(2))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Conv1D(128, 8, activation='relu', padding='same'))\n",
    "    #model.add(MaxPooling1D(2))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Conv1D(128, 8, activation='relu', padding='same'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(a):\n",
    "    return sum(a) / len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DPCNN1():\n",
    "    filter_nr = 128 #滤波器通道个数\n",
    "    filter_size = 3 #卷积核\n",
    "    max_pool_size = 3 #池化层的pooling_size\n",
    "    max_pool_strides = 2 #池化层的步长\n",
    "    dense_nr = 256 #全连接层\n",
    "    spatial_dropout = 0.2\n",
    "    dense_dropout = 0.5\n",
    "    train_embed = False\n",
    "    conv_kern_reg = regularizers.l2(0.00001)\n",
    "    conv_bias_reg = regularizers.l2(0.00001)\n",
    "\n",
    "    comment = Input(shape=(maxlen,))\n",
    "#     emb_comment = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=train_embed)(comment)\n",
    "    emb_comment = Embedding(max_features, embed_size, trainable=train_embed)(comment)\n",
    "    emb_comment = SpatialDropout1D(spatial_dropout)(emb_comment)\n",
    "\n",
    "    block1 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n",
    "                kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(emb_comment)\n",
    "    block1 = BatchNormalization()(block1)\n",
    "    block1 = PReLU()(block1)\n",
    "    block1 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n",
    "                kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block1)\n",
    "    block1 = BatchNormalization()(block1)\n",
    "    block1 = PReLU()(block1)\n",
    "\n",
    "    #we pass embedded comment through conv1d with filter size 1 because it needs to have the same shape as block output\n",
    "    #if you choose filter_nr = embed_size (300 in this case) you don't have to do this part and can add emb_comment directly to block1_output\n",
    "    resize_emb = Conv1D(filter_nr, kernel_size=1, padding='same', activation='linear', \n",
    "                kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(emb_comment)\n",
    "    resize_emb = PReLU()(resize_emb)\n",
    "\n",
    "    \n",
    "    \n",
    "    block1_output = add([block1, resize_emb])\n",
    "    block1_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block1_output)\n",
    "\n",
    "    block2 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n",
    "                kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block1_output)\n",
    "    block2 = BatchNormalization()(block2)\n",
    "    block2 = PReLU()(block2)\n",
    "    block2 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n",
    "                kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block2)\n",
    "    block2 = BatchNormalization()(block2)\n",
    "    block2 = PReLU()(block2)\n",
    "\n",
    "    block2_output = add([block2, block1_output])\n",
    "    block2_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block2_output)\n",
    "\n",
    "    \n",
    "    \n",
    "    block3 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n",
    "                kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block2_output)\n",
    "    block3 = BatchNormalization()(block3)\n",
    "    block3 = PReLU()(block3)\n",
    "    block3 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n",
    "                kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block3)\n",
    "    block3 = BatchNormalization()(block3)\n",
    "    block3 = PReLU()(block3)\n",
    "\n",
    "    block3_output = add([block3, block2_output])\n",
    "    block3_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block3_output)\n",
    "\n",
    "    \n",
    "    block4 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n",
    "                kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block3_output)\n",
    "    block4 = BatchNormalization()(block4)\n",
    "    block4 = PReLU()(block4)\n",
    "    block4 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n",
    "                kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block4)\n",
    "    block4 = BatchNormalization()(block4)\n",
    "    block4 = PReLU()(block4)\n",
    "\n",
    "    block4_output = add([block4, block3_output])\n",
    "    block4_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block4_output)\n",
    "    \n",
    "    \n",
    "    block5 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n",
    "                kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block4_output)\n",
    "    block5 = BatchNormalization()(block5)\n",
    "    block5 = PReLU()(block5)\n",
    "    block5 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n",
    "                kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block5)\n",
    "    block5 = BatchNormalization()(block5)\n",
    "    block5 = PReLU()(block5)\n",
    "\n",
    "    block5_output = add([block5, block4_output])\n",
    "    block5_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block5_output)\n",
    "\n",
    "    block6 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n",
    "                kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block5_output)\n",
    "    block6 = BatchNormalization()(block6)\n",
    "    block6 = PReLU()(block6)\n",
    "    block6 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n",
    "                kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block6)\n",
    "    block6 = BatchNormalization()(block6)\n",
    "    block6 = PReLU()(block6)\n",
    "\n",
    "    block6_output = add([block6, block5_output])\n",
    "    block6_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block6_output)\n",
    "\n",
    "    block7 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n",
    "                kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block6_output)\n",
    "    block7 = BatchNormalization()(block7)\n",
    "    block7 = PReLU()(block7)\n",
    "    block7 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n",
    "                kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block7)\n",
    "    block7 = BatchNormalization()(block7)\n",
    "    block7 = PReLU()(block7)\n",
    "\n",
    "    block7_output = add([block7, block6_output])\n",
    "    output = GlobalMaxPooling1D()(block7_output)\n",
    "\n",
    "    output = Dense(dense_nr, activation='linear')(output)\n",
    "    output = BatchNormalization()(output)\n",
    "    output = PReLU()(output)\n",
    "    output = Dropout(dense_dropout)(output)\n",
    "    output = Dense(8, activation='softmax')(output)\n",
    "\n",
    "    model = Model(comment, output)\n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN(x):\n",
    "    \n",
    "    block = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n",
    "                kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(x)\n",
    "    block = BatchNormalization()(block)\n",
    "    block = PReLU()(block)\n",
    "    block = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear', \n",
    "                kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(block)\n",
    "    block = BatchNormalization()(block)\n",
    "    block = PReLU()(block)\n",
    "    return block\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "filter_nr = 64 #滤波器通道个数\n",
    "filter_size = 3 #卷积核\n",
    "max_pool_size = 3 #池化层的pooling_size\n",
    "max_pool_strides = 2 #池化层的步长\n",
    "dense_nr = 256 #全连接层\n",
    "spatial_dropout = 0.2\n",
    "dense_dropout = 0.5\n",
    "train_embed = False\n",
    "conv_kern_reg = regularizers.l2(0.00001)\n",
    "conv_bias_reg = regularizers.l2(0.00001)\n",
    "\n",
    "\n",
    "filter_nr = 128\n",
    "filter_size = 3\n",
    "max_pool_size = 3\n",
    "max_pool_strides = 2\n",
    "dense_nr = 256\n",
    "spatial_dropout = 0.4\n",
    "dense_dropout = 0.4\n",
    "train_embed = False\n",
    "    \n",
    "def DPCNN2():\n",
    "    comment = Input(shape=(maxlen,))\n",
    "#     emb_comment = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=train_embed)(comment)\n",
    "    emb_comment = Embedding(max_features, embed_size,trainable=train_embed)(comment)\n",
    "    emb_comment = SpatialDropout1D(spatial_dropout)(emb_comment)\n",
    "\n",
    "    #region embedding层\n",
    "    resize_emb = Conv1D(filter_nr, kernel_size=1,padding='same', activation='linear',  \n",
    "                kernel_regularizer=conv_kern_reg, bias_regularizer=conv_bias_reg)(emb_comment)\n",
    "    resize_emb = PReLU()(resize_emb)\n",
    "    #第一层\n",
    "    block1 = CNN(emb_comment)\n",
    "    block1_output = add([block1, resize_emb])\n",
    "    block1_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block1_output)\n",
    "    #第二层\n",
    "    block2 = CNN(block1_output)\n",
    "    block2_output = add([block2, block1_output])\n",
    "    block2_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block2_output)\n",
    "    #第三层\n",
    "    block3 = CNN(block2_output)\n",
    "    block3_output = add([block3, block2_output])\n",
    "    block3_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block3_output)  \n",
    "    #第四层\n",
    "    block4 = CNN(block3_output) \n",
    "    block4_output = add([block4, block3_output])\n",
    "    block4_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block4_output)\n",
    "    #第五层\n",
    "    block5 = CNN(block4_output) \n",
    "    block5_output = add([block5, block4_output])\n",
    "    block5_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block5_output)\n",
    "    #第六层\n",
    "    block6 = CNN(block5_output) \n",
    "    block6_output = add([block6, block5_output])\n",
    "    block6_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(block6_output)\n",
    "    #第七层\n",
    "    block7 = CNN(block6_output) \n",
    "    block7_output = add([block7, block6_output])\n",
    "    output = GlobalMaxPooling1D()(block7_output)\n",
    "    #全连接层\n",
    "    output = Dense(dense_nr, activation=\"linear\")(output)\n",
    "    output = BatchNormalization()(output)\n",
    "    output = PReLU()(output)\n",
    "    output = Dropout(dense_dropout)(output)\n",
    "    output = Dense(8, activation=\"softmax\")(output)\n",
    "\n",
    "    model = Model(comment, output)\n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DPCNN3():\n",
    "    project_maxlen = 1000\n",
    "    \n",
    "    filter_nr = 128\n",
    "    filter_size = 3\n",
    "    max_pool_size = 3\n",
    "    max_pool_strides = 2\n",
    "    dense_nr = 256\n",
    "    spatial_dropout = 0.4\n",
    "    dense_dropout = 0.3\n",
    "    train_embed = False\n",
    "    \n",
    "    pj_repeat = 5     # dpcnn block repeated times on project text \n",
    "\n",
    "    project = Input(shape=(project_maxlen,), name='project')\n",
    "    emb_project = Embedding(max_features, embed_size, trainable=train_embed)(project)\n",
    "#     emb_project = Embedding(304, 256)(project)\n",
    "    emb_project = SpatialDropout1D(spatial_dropout)(emb_project)\n",
    "    pj_block1 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(emb_project)\n",
    "    pj_block1 = BatchNormalization()(pj_block1)\n",
    "    pj_block1 = PReLU()(pj_block1)\n",
    "    pj_block1 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(pj_block1)\n",
    "    pj_block1 = BatchNormalization()(pj_block1)\n",
    "    pj_block1 = PReLU()(pj_block1)\n",
    "    pj_resize_emb = Conv1D(filter_nr, kernel_size=1, padding='same', activation='linear')(emb_project)\n",
    "    pj_resize_emb = PReLU()(pj_resize_emb)\n",
    "    pj_block1_output = add([pj_block1, pj_resize_emb])\n",
    "    for _ in range(pj_repeat):  \n",
    "        pj_block1_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(pj_block1_output)\n",
    "        pj_block2 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(pj_block1_output)\n",
    "        pj_block2 = BatchNormalization()(pj_block2)\n",
    "        pj_block2 = PReLU()(pj_block2)\n",
    "        pj_block2 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(pj_block2)\n",
    "        pj_block2 = BatchNormalization()(pj_block2)\n",
    "        pj_block2 = PReLU()(pj_block2)\n",
    "        pj_block1_output = add([pj_block2, pj_block1_output])\n",
    "    \n",
    "        \n",
    "    pj_output = GlobalMaxPooling1D()(pj_block1_output)\n",
    "    pj_output = BatchNormalization()(pj_output)\n",
    "    \n",
    "    \n",
    "    output = Dense(dense_nr, activation='linear')(pj_output)\n",
    "    output = BatchNormalization()(output)\n",
    "    output = PReLU()(output)\n",
    "    output = Dropout(dense_dropout)(output)\n",
    "    output = Dense(8, activation='softmax')(output)\n",
    "    model = Model(inputs=project, outputs=output)\n",
    "#     model.compile(loss='binary_crossentropy', \n",
    "#                 optimizer='adam',\n",
    "#                 metrics=['accuracy'])\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project  resources  train_seq\n",
    "# 2st DPCNN + Pseudo-Labelling\n",
    "def get_model2():\n",
    "#     session_conf = tf.ConfigProto(intra_op_parallelism_threads=4, inter_op_parallelism_threads=4)\n",
    "#     K.set_session(tf.Session(graph=tf.get_default_graph(), config=session_conf))\n",
    "\n",
    "    filter_nr = 128\n",
    "    filter_size = 3\n",
    "    max_pool_size = 3\n",
    "    max_pool_strides = 2\n",
    "    dense_nr = 256\n",
    "    spatial_dropout = 0.4\n",
    "    dense_dropout = 0.3\n",
    "    train_embed = False\n",
    "    \n",
    "    pj_repeat = 5     # dpcnn block repeated times on project text \n",
    "    rs_repeat = 1     # dpcnn block repeated times on resources text \n",
    "\n",
    "    project = Input(shape=(project_maxlen,), name='project')\n",
    "#     emb_project = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=train_embed)(project)\n",
    "    emb_project = Embedding(max_features, embed_size, trainable=train_embed)(project)\n",
    "    emb_project = SpatialDropout1D(spatial_dropout)(emb_project)\n",
    "    pj_block1 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(emb_project)\n",
    "    pj_block1 = BatchNormalization()(pj_block1)\n",
    "    pj_block1 = PReLU()(pj_block1)\n",
    "    pj_block1 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(pj_block1)\n",
    "    pj_block1 = BatchNormalization()(pj_block1)\n",
    "    pj_block1 = PReLU()(pj_block1)\n",
    "    pj_resize_emb = Conv1D(filter_nr, kernel_size=1, padding='same', activation='linear')(emb_project)\n",
    "    pj_resize_emb = PReLU()(pj_resize_emb)\n",
    "    pj_block1_output = add([pj_block1, pj_resize_emb])\n",
    "    for _ in range(pj_repeat):  \n",
    "        pj_block1_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(pj_block1_output)\n",
    "        pj_block2 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(pj_block1_output)\n",
    "        pj_block2 = BatchNormalization()(pj_block2)\n",
    "        pj_block2 = PReLU()(pj_block2)\n",
    "        pj_block2 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(pj_block2)\n",
    "        pj_block2 = BatchNormalization()(pj_block2)\n",
    "        pj_block2 = PReLU()(pj_block2)\n",
    "        pj_block1_output = add([pj_block2, pj_block1_output])\n",
    "    \n",
    "    resouse = Input(shape=(resouse_max_len,), name='resouse')\n",
    "#     emb_resouse = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=train_embed)(resouse)\n",
    "    emb_project = Embedding(max_features, embed_size, trainable=train_embed)(project)\n",
    "    emb_resouse = SpatialDropout1D(spatial_dropout)(emb_resouse)\n",
    "    rs_block1 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(emb_resouse)\n",
    "    rs_block1 = BatchNormalization()(rs_block1)\n",
    "    rs_block1 = PReLU()(rs_block1)\n",
    "    rs_block1 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(rs_block1)\n",
    "    rs_block1 = BatchNormalization()(rs_block1)\n",
    "    rs_block1 = PReLU()(rs_block1)\n",
    "    rs_resize_emb = Conv1D(filter_nr, kernel_size=1, padding='same', activation='linear')(emb_resouse)\n",
    "    rs_resize_emb = PReLU()(rs_resize_emb)\n",
    "\n",
    "    rs_block1_output = add([rs_block1, rs_resize_emb])\n",
    "    for _ in range(rs_repeat):  \n",
    "        rs_block1_output = MaxPooling1D(pool_size=max_pool_size, strides=max_pool_strides)(rs_block1_output)\n",
    "        rs_block2 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(rs_block1_output)\n",
    "        rs_block2 = BatchNormalization()(rs_block2)\n",
    "        rs_block2 = PReLU()(rs_block2)\n",
    "        rs_block2 = Conv1D(filter_nr, kernel_size=filter_size, padding='same', activation='linear')(rs_block2)\n",
    "        rs_block2 = BatchNormalization()(rs_block2)\n",
    "        rs_block2 = PReLU()(rs_block2)\n",
    "        rs_block1_output = add([rs_block2, rs_block1_output])\n",
    "        \n",
    "    pj_output = GlobalMaxPooling1D()(pj_block1_output)\n",
    "    rs_output = GlobalMaxPooling1D()(rs_block1_output)\n",
    "    pj_output = BatchNormalization()(pj_output)\n",
    "    rs_output = BatchNormalization()(rs_output)\n",
    "    \n",
    "    inp_num = Input(shape=(train_seq.shape[1]-maxlen, ), name='num_input')\n",
    "    conc = concatenate([pj_output, rs_output, inp_num])\n",
    "    \n",
    "    output = Dense(dense_nr, activation='linear')(conc)\n",
    "    output = BatchNormalization()(output)\n",
    "    output = PReLU()(output)\n",
    "    output = Dropout(dense_dropout)(output)\n",
    "    output = Dense(1, activation='sigmoid')(output)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[project, resouse, inp_num], outputs=output)\n",
    "#     model.compile(loss='binary_crossentropy', \n",
    "#                 optimizer='adam',\n",
    "#                 metrics=['accuracy'])\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_model2():\n",
    "    S_inputs = Input(shape=(None,), dtype='int32')\n",
    "    embeddings = Embedding(304, 256)(S_inputs)\n",
    "    embeddings = Position_Embedding()(embeddings) # 增加Position_Embedding能轻微提高准确率\n",
    "    O_seq = Attention(8,16)([embeddings,embeddings,embeddings])\n",
    "    O_seq = GlobalAveragePooling1D()(O_seq)\n",
    "    O_seq = Dropout(0.5)(O_seq)\n",
    "    outputs = Dense(8, activation='softmax')(O_seq)\n",
    "\n",
    "    model = Model(inputs=S_inputs, outputs=outputs)\n",
    "    # try using different optimizers and different optimizer configs\n",
    "    # model.compile(loss='binary_crossentropy',\n",
    "    #               optimizer='adam',\n",
    "    #               metrics=['accuracy'])\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - done in {:.2f}s\".format(title, time.time() - t0))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 66413 samples, validate on 7380 samples\n",
      "Epoch 1/200\n",
      "66413/66413 [==============================] - 48s 723us/step - loss: 0.3023 - acc: 0.9084 - val_loss: 0.2735 - val_acc: 0.9077\n",
      "Epoch 2/200\n",
      "66413/66413 [==============================] - 2s 37us/step - loss: 0.2644 - acc: 0.9096 - val_loss: 0.2610 - val_acc: 0.9077\n",
      "Epoch 3/200\n",
      "66413/66413 [==============================] - 3s 38us/step - loss: 0.2539 - acc: 0.9096 - val_loss: 0.2484 - val_acc: 0.9077\n",
      "Epoch 4/200\n",
      "66413/66413 [==============================] - 2s 37us/step - loss: 0.2464 - acc: 0.9096 - val_loss: 0.2480 - val_acc: 0.9077\n",
      "Epoch 5/200\n",
      "66413/66413 [==============================] - 2s 38us/step - loss: 0.2413 - acc: 0.9093 - val_loss: 0.2418 - val_acc: 0.9077\n",
      "Epoch 6/200\n",
      "66413/66413 [==============================] - 2s 38us/step - loss: 0.2388 - acc: 0.9094 - val_loss: 0.2377 - val_acc: 0.9080\n",
      "Epoch 7/200\n",
      "66413/66413 [==============================] - 2s 37us/step - loss: 0.2371 - acc: 0.9096 - val_loss: 0.2362 - val_acc: 0.9084\n",
      "Epoch 8/200\n",
      "66413/66413 [==============================] - 3s 38us/step - loss: 0.2348 - acc: 0.9097 - val_loss: 0.2350 - val_acc: 0.9081\n",
      "Epoch 9/200\n",
      "66413/66413 [==============================] - 3s 39us/step - loss: 0.2318 - acc: 0.9098 - val_loss: 0.2340 - val_acc: 0.9083\n",
      "Epoch 10/200\n",
      "66413/66413 [==============================] - 3s 39us/step - loss: 0.2289 - acc: 0.9091 - val_loss: 0.2314 - val_acc: 0.9087\n",
      "Epoch 11/200\n",
      "66413/66413 [==============================] - 3s 38us/step - loss: 0.2265 - acc: 0.9107 - val_loss: 0.2356 - val_acc: 0.9079\n",
      "Epoch 12/200\n",
      "66413/66413 [==============================] - 3s 38us/step - loss: 0.2255 - acc: 0.9105 - val_loss: 0.2357 - val_acc: 0.9065\n",
      "Epoch 13/200\n",
      "66413/66413 [==============================] - 2s 37us/step - loss: 0.2233 - acc: 0.9107 - val_loss: 0.2325 - val_acc: 0.9081\n",
      "AUC: 0.861911\n",
      "Train on 66413 samples, validate on 7380 samples\n",
      "Epoch 1/200\n",
      "66413/66413 [==============================] - 4s 56us/step - loss: 0.3066 - acc: 0.9082 - val_loss: 0.2714 - val_acc: 0.9123\n",
      "Epoch 2/200\n",
      "66413/66413 [==============================] - 2s 38us/step - loss: 0.2676 - acc: 0.9090 - val_loss: 0.2544 - val_acc: 0.9123\n",
      "Epoch 3/200\n",
      "66413/66413 [==============================] - 3s 39us/step - loss: 0.2551 - acc: 0.9090 - val_loss: 0.2642 - val_acc: 0.9123\n",
      "Epoch 4/200\n",
      "66413/66413 [==============================] - 3s 40us/step - loss: 0.2492 - acc: 0.9089 - val_loss: 0.2418 - val_acc: 0.9123\n",
      "Epoch 5/200\n",
      "66413/66413 [==============================] - 3s 39us/step - loss: 0.2451 - acc: 0.9089 - val_loss: 0.2350 - val_acc: 0.9122\n",
      "Epoch 6/200\n",
      "66413/66413 [==============================] - 3s 40us/step - loss: 0.2412 - acc: 0.9091 - val_loss: 0.2376 - val_acc: 0.9126\n",
      "Epoch 7/200\n",
      "66413/66413 [==============================] - 2s 37us/step - loss: 0.2390 - acc: 0.9089 - val_loss: 0.2316 - val_acc: 0.9130\n",
      "Epoch 8/200\n",
      "66413/66413 [==============================] - 2s 37us/step - loss: 0.2367 - acc: 0.9086 - val_loss: 0.2292 - val_acc: 0.9129\n",
      "Epoch 9/200\n",
      "66413/66413 [==============================] - 3s 38us/step - loss: 0.2354 - acc: 0.9092 - val_loss: 0.2279 - val_acc: 0.9127\n",
      "Epoch 10/200\n",
      "66413/66413 [==============================] - 2s 38us/step - loss: 0.2319 - acc: 0.9094 - val_loss: 0.2270 - val_acc: 0.9130\n",
      "Epoch 11/200\n",
      "66413/66413 [==============================] - 3s 38us/step - loss: 0.2288 - acc: 0.9093 - val_loss: 0.2237 - val_acc: 0.9130\n",
      "Epoch 12/200\n",
      "66413/66413 [==============================] - 3s 38us/step - loss: 0.2283 - acc: 0.9100 - val_loss: 0.2244 - val_acc: 0.9127\n",
      "Epoch 13/200\n",
      "66413/66413 [==============================] - 3s 38us/step - loss: 0.2277 - acc: 0.9094 - val_loss: 0.2248 - val_acc: 0.9136\n",
      "Epoch 14/200\n",
      "66413/66413 [==============================] - 3s 38us/step - loss: 0.2252 - acc: 0.9096 - val_loss: 0.2229 - val_acc: 0.9144\n",
      "Epoch 15/200\n",
      "66413/66413 [==============================] - 3s 38us/step - loss: 0.2239 - acc: 0.9104 - val_loss: 0.2220 - val_acc: 0.9138\n",
      "Epoch 16/200\n",
      "66413/66413 [==============================] - 3s 38us/step - loss: 0.2218 - acc: 0.9105 - val_loss: 0.2217 - val_acc: 0.9138\n",
      "Epoch 17/200\n",
      "66413/66413 [==============================] - 2s 37us/step - loss: 0.2205 - acc: 0.9106 - val_loss: 0.2325 - val_acc: 0.9133\n",
      "Epoch 18/200\n",
      "66413/66413 [==============================] - 3s 39us/step - loss: 0.2195 - acc: 0.9105 - val_loss: 0.2380 - val_acc: 0.9103\n",
      "Epoch 19/200\n",
      "66413/66413 [==============================] - 3s 38us/step - loss: 0.2192 - acc: 0.9118 - val_loss: 0.2231 - val_acc: 0.9136\n",
      "AUC: 0.861559\n",
      "Train on 66413 samples, validate on 7380 samples\n",
      "Epoch 1/200\n",
      "66413/66413 [==============================] - 4s 56us/step - loss: 0.3051 - acc: 0.9081 - val_loss: 0.2754 - val_acc: 0.9092\n",
      "Epoch 2/200\n",
      "66413/66413 [==============================] - 3s 38us/step - loss: 0.2669 - acc: 0.9094 - val_loss: 0.2526 - val_acc: 0.9092\n",
      "Epoch 3/200\n",
      "66413/66413 [==============================] - 3s 38us/step - loss: 0.2561 - acc: 0.9094 - val_loss: 0.2499 - val_acc: 0.9092\n",
      "Epoch 4/200\n",
      "66413/66413 [==============================] - 2s 36us/step - loss: 0.2486 - acc: 0.9092 - val_loss: 0.2443 - val_acc: 0.9092\n",
      "Epoch 5/200\n",
      "66413/66413 [==============================] - 2s 36us/step - loss: 0.2448 - acc: 0.9093 - val_loss: 0.2441 - val_acc: 0.9092\n",
      "Epoch 6/200\n",
      "66413/66413 [==============================] - 2s 37us/step - loss: 0.2425 - acc: 0.9095 - val_loss: 0.2409 - val_acc: 0.9093\n",
      "Epoch 7/200\n",
      "66413/66413 [==============================] - 2s 37us/step - loss: 0.2366 - acc: 0.9097 - val_loss: 0.2324 - val_acc: 0.9091\n",
      "Epoch 8/200\n",
      "66413/66413 [==============================] - 2s 37us/step - loss: 0.2332 - acc: 0.9100 - val_loss: 0.2312 - val_acc: 0.9100\n",
      "Epoch 9/200\n",
      "66413/66413 [==============================] - 2s 36us/step - loss: 0.2311 - acc: 0.9096 - val_loss: 0.2378 - val_acc: 0.9081\n",
      "Epoch 10/200\n",
      "66413/66413 [==============================] - 2s 36us/step - loss: 0.2289 - acc: 0.9100 - val_loss: 0.2297 - val_acc: 0.9095\n",
      "Epoch 11/200\n",
      "66413/66413 [==============================] - 2s 37us/step - loss: 0.2268 - acc: 0.9101 - val_loss: 0.2269 - val_acc: 0.9100\n",
      "Epoch 12/200\n",
      "66413/66413 [==============================] - 3s 38us/step - loss: 0.2250 - acc: 0.9099 - val_loss: 0.2318 - val_acc: 0.9102\n",
      "Epoch 13/200\n",
      "66413/66413 [==============================] - 2s 37us/step - loss: 0.2236 - acc: 0.9107 - val_loss: 0.2261 - val_acc: 0.9107\n",
      "Epoch 14/200\n",
      "66413/66413 [==============================] - 3s 38us/step - loss: 0.2232 - acc: 0.9105 - val_loss: 0.2304 - val_acc: 0.9115\n",
      "Epoch 15/200\n",
      "66413/66413 [==============================] - 2s 37us/step - loss: 0.2202 - acc: 0.9107 - val_loss: 0.2281 - val_acc: 0.9104\n",
      "Epoch 16/200\n",
      "66413/66413 [==============================] - 2s 36us/step - loss: 0.2183 - acc: 0.9108 - val_loss: 0.2254 - val_acc: 0.9104\n",
      "Epoch 17/200\n",
      "66413/66413 [==============================] - 2s 37us/step - loss: 0.2177 - acc: 0.9119 - val_loss: 0.2300 - val_acc: 0.9076\n",
      "Epoch 18/200\n",
      "66413/66413 [==============================] - 2s 38us/step - loss: 0.2178 - acc: 0.9116 - val_loss: 0.2500 - val_acc: 0.9049\n",
      "Epoch 19/200\n",
      "66413/66413 [==============================] - 3s 38us/step - loss: 0.2142 - acc: 0.9114 - val_loss: 0.2272 - val_acc: 0.9107\n",
      "AUC: 0.863474\n",
      "Train on 66414 samples, validate on 7379 samples\n",
      "Epoch 1/200\n",
      "66414/66414 [==============================] - 4s 59us/step - loss: 0.3054 - acc: 0.9086 - val_loss: 0.2883 - val_acc: 0.9053\n",
      "Epoch 2/200\n",
      "66414/66414 [==============================] - 3s 38us/step - loss: 0.2679 - acc: 0.9098 - val_loss: 0.2647 - val_acc: 0.9053\n",
      "Epoch 3/200\n",
      "66414/66414 [==============================] - 3s 38us/step - loss: 0.2550 - acc: 0.9098 - val_loss: 0.2543 - val_acc: 0.9053\n",
      "Epoch 4/200\n",
      "66414/66414 [==============================] - 3s 38us/step - loss: 0.2481 - acc: 0.9095 - val_loss: 0.2425 - val_acc: 0.9053\n",
      "Epoch 5/200\n",
      "66414/66414 [==============================] - 3s 39us/step - loss: 0.2424 - acc: 0.9096 - val_loss: 0.2399 - val_acc: 0.9057\n",
      "Epoch 6/200\n",
      "66414/66414 [==============================] - 2s 38us/step - loss: 0.2369 - acc: 0.9096 - val_loss: 0.2318 - val_acc: 0.9057\n",
      "Epoch 7/200\n",
      "66414/66414 [==============================] - 3s 39us/step - loss: 0.2339 - acc: 0.9100 - val_loss: 0.2302 - val_acc: 0.9062\n",
      "Epoch 8/200\n",
      "66414/66414 [==============================] - 3s 39us/step - loss: 0.2321 - acc: 0.9100 - val_loss: 0.2307 - val_acc: 0.9055\n",
      "Epoch 9/200\n",
      "66414/66414 [==============================] - 3s 38us/step - loss: 0.2303 - acc: 0.9098 - val_loss: 0.2332 - val_acc: 0.9062\n",
      "Epoch 10/200\n",
      "66414/66414 [==============================] - 3s 38us/step - loss: 0.2284 - acc: 0.9103 - val_loss: 0.2328 - val_acc: 0.9068\n",
      "AUC: 0.867502\n",
      "Train on 66414 samples, validate on 7379 samples\n",
      "Epoch 1/200\n",
      "66414/66414 [==============================] - 4s 60us/step - loss: 0.3076 - acc: 0.9093 - val_loss: 0.2862 - val_acc: 0.9064\n",
      "Epoch 2/200\n",
      "66414/66414 [==============================] - 3s 39us/step - loss: 0.2678 - acc: 0.9097 - val_loss: 0.2570 - val_acc: 0.9064\n",
      "Epoch 3/200\n",
      "66414/66414 [==============================] - 3s 39us/step - loss: 0.2531 - acc: 0.9097 - val_loss: 0.2494 - val_acc: 0.9064\n",
      "Epoch 4/200\n",
      "66414/66414 [==============================] - 3s 39us/step - loss: 0.2461 - acc: 0.9095 - val_loss: 0.2361 - val_acc: 0.9069\n",
      "Epoch 5/200\n",
      "66414/66414 [==============================] - 3s 39us/step - loss: 0.2423 - acc: 0.9093 - val_loss: 0.2340 - val_acc: 0.9064\n",
      "Epoch 6/200\n",
      "66414/66414 [==============================] - 2s 37us/step - loss: 0.2379 - acc: 0.9095 - val_loss: 0.2355 - val_acc: 0.9074\n",
      "Epoch 7/200\n",
      "66414/66414 [==============================] - 2s 37us/step - loss: 0.2354 - acc: 0.9094 - val_loss: 0.2305 - val_acc: 0.9069\n",
      "Epoch 8/200\n",
      "66414/66414 [==============================] - 3s 38us/step - loss: 0.2334 - acc: 0.9098 - val_loss: 0.2430 - val_acc: 0.9080\n",
      "Epoch 9/200\n",
      "66414/66414 [==============================] - 3s 38us/step - loss: 0.2312 - acc: 0.9097 - val_loss: 0.2296 - val_acc: 0.9074\n",
      "Epoch 10/200\n",
      "66414/66414 [==============================] - 3s 40us/step - loss: 0.2294 - acc: 0.9100 - val_loss: 0.2339 - val_acc: 0.9080\n",
      "Epoch 11/200\n",
      "66414/66414 [==============================] - 3s 38us/step - loss: 0.2256 - acc: 0.9105 - val_loss: 0.2300 - val_acc: 0.9069\n",
      "Epoch 12/200\n",
      "66414/66414 [==============================] - 3s 38us/step - loss: 0.2254 - acc: 0.9110 - val_loss: 0.2299 - val_acc: 0.9072\n",
      "AUC: 0.872636\n",
      "Train on 66414 samples, validate on 7379 samples\n",
      "Epoch 1/200\n",
      "66414/66414 [==============================] - 4s 60us/step - loss: 0.3061 - acc: 0.9069 - val_loss: 0.2702 - val_acc: 0.9125\n",
      "Epoch 2/200\n",
      "66414/66414 [==============================] - 3s 38us/step - loss: 0.2647 - acc: 0.9090 - val_loss: 0.2476 - val_acc: 0.9125\n",
      "Epoch 3/200\n",
      "66414/66414 [==============================] - 3s 39us/step - loss: 0.2519 - acc: 0.9090 - val_loss: 0.2447 - val_acc: 0.9122\n",
      "Epoch 4/200\n",
      "66414/66414 [==============================] - 2s 38us/step - loss: 0.2453 - acc: 0.9090 - val_loss: 0.2386 - val_acc: 0.9123\n",
      "Epoch 5/200\n",
      "66414/66414 [==============================] - 3s 39us/step - loss: 0.2394 - acc: 0.9091 - val_loss: 0.2358 - val_acc: 0.9127\n",
      "Epoch 6/200\n",
      "66414/66414 [==============================] - 3s 38us/step - loss: 0.2369 - acc: 0.9089 - val_loss: 0.2316 - val_acc: 0.9129\n",
      "Epoch 7/200\n",
      "66414/66414 [==============================] - 3s 38us/step - loss: 0.2334 - acc: 0.9096 - val_loss: 0.2316 - val_acc: 0.9118\n",
      "Epoch 8/200\n",
      "66414/66414 [==============================] - 3s 38us/step - loss: 0.2317 - acc: 0.9091 - val_loss: 0.2326 - val_acc: 0.9119\n",
      "Epoch 9/200\n",
      "66414/66414 [==============================] - 3s 38us/step - loss: 0.2285 - acc: 0.9093 - val_loss: 0.2335 - val_acc: 0.9099\n",
      "AUC: 0.850394\n",
      "Train on 66414 samples, validate on 7379 samples\n",
      "Epoch 1/200\n",
      "66414/66414 [==============================] - 4s 63us/step - loss: 0.3119 - acc: 0.9069 - val_loss: 0.3049 - val_acc: 0.9068\n",
      "Epoch 2/200\n",
      "66414/66414 [==============================] - 3s 40us/step - loss: 0.2796 - acc: 0.9097 - val_loss: 0.2650 - val_acc: 0.9068\n",
      "Epoch 3/200\n",
      "66414/66414 [==============================] - 3s 39us/step - loss: 0.2590 - acc: 0.9096 - val_loss: 0.2473 - val_acc: 0.9068\n",
      "Epoch 4/200\n",
      "66414/66414 [==============================] - 3s 39us/step - loss: 0.2488 - acc: 0.9096 - val_loss: 0.2515 - val_acc: 0.9062\n",
      "Epoch 5/200\n",
      "66414/66414 [==============================] - 3s 39us/step - loss: 0.2428 - acc: 0.9094 - val_loss: 0.2404 - val_acc: 0.9066\n",
      "Epoch 6/200\n",
      "66414/66414 [==============================] - 3s 39us/step - loss: 0.2386 - acc: 0.9093 - val_loss: 0.2385 - val_acc: 0.9068\n",
      "Epoch 7/200\n",
      "66414/66414 [==============================] - 3s 40us/step - loss: 0.2339 - acc: 0.9101 - val_loss: 0.2304 - val_acc: 0.9069\n",
      "Epoch 8/200\n",
      "66414/66414 [==============================] - 3s 41us/step - loss: 0.2327 - acc: 0.9096 - val_loss: 0.2333 - val_acc: 0.9064\n",
      "Epoch 9/200\n",
      "66414/66414 [==============================] - 3s 40us/step - loss: 0.2297 - acc: 0.9095 - val_loss: 0.2302 - val_acc: 0.9064\n",
      "Epoch 10/200\n",
      "66414/66414 [==============================] - 3s 39us/step - loss: 0.2278 - acc: 0.9105 - val_loss: 0.2298 - val_acc: 0.9072\n",
      "Epoch 11/200\n",
      "66414/66414 [==============================] - 3s 40us/step - loss: 0.2261 - acc: 0.9110 - val_loss: 0.2267 - val_acc: 0.9066\n",
      "Epoch 12/200\n",
      "66414/66414 [==============================] - 3s 40us/step - loss: 0.2246 - acc: 0.9106 - val_loss: 0.2263 - val_acc: 0.9080\n",
      "Epoch 13/200\n",
      "66414/66414 [==============================] - 3s 41us/step - loss: 0.2229 - acc: 0.9110 - val_loss: 0.2313 - val_acc: 0.9070\n",
      "Epoch 14/200\n",
      "66414/66414 [==============================] - 3s 40us/step - loss: 0.2213 - acc: 0.9107 - val_loss: 0.2307 - val_acc: 0.9054\n",
      "Epoch 15/200\n",
      "66414/66414 [==============================] - 3s 40us/step - loss: 0.2200 - acc: 0.9118 - val_loss: 0.2266 - val_acc: 0.9054\n",
      "AUC: 0.866981\n",
      "Train on 66414 samples, validate on 7379 samples\n",
      "Epoch 1/200\n",
      "66414/66414 [==============================] - 4s 64us/step - loss: 0.3053 - acc: 0.9082 - val_loss: 0.2763 - val_acc: 0.9087\n",
      "Epoch 2/200\n",
      "66414/66414 [==============================] - 3s 38us/step - loss: 0.2664 - acc: 0.9095 - val_loss: 0.2568 - val_acc: 0.9087\n",
      "Epoch 3/200\n",
      "66414/66414 [==============================] - 2s 38us/step - loss: 0.2509 - acc: 0.9094 - val_loss: 0.2378 - val_acc: 0.9085\n",
      "Epoch 4/200\n",
      "66414/66414 [==============================] - 3s 40us/step - loss: 0.2443 - acc: 0.9094 - val_loss: 0.2337 - val_acc: 0.9087\n",
      "Epoch 5/200\n",
      "66414/66414 [==============================] - 3s 39us/step - loss: 0.2381 - acc: 0.9096 - val_loss: 0.2302 - val_acc: 0.9085\n",
      "Epoch 6/200\n",
      "66414/66414 [==============================] - 3s 39us/step - loss: 0.2353 - acc: 0.9092 - val_loss: 0.2261 - val_acc: 0.9095\n",
      "Epoch 7/200\n",
      "66414/66414 [==============================] - 3s 39us/step - loss: 0.2325 - acc: 0.9098 - val_loss: 0.2268 - val_acc: 0.9091\n",
      "Epoch 8/200\n",
      "66414/66414 [==============================] - 3s 38us/step - loss: 0.2307 - acc: 0.9099 - val_loss: 0.2307 - val_acc: 0.9096\n",
      "Epoch 9/200\n",
      "66414/66414 [==============================] - 3s 38us/step - loss: 0.2285 - acc: 0.9098 - val_loss: 0.2275 - val_acc: 0.9093\n",
      "AUC: 0.867243\n",
      "Train on 66414 samples, validate on 7379 samples\n",
      "Epoch 1/200\n",
      "66414/66414 [==============================] - 4s 63us/step - loss: 0.3078 - acc: 0.9093 - val_loss: 0.2799 - val_acc: 0.9088\n",
      "Epoch 2/200\n",
      "66414/66414 [==============================] - 3s 38us/step - loss: 0.2694 - acc: 0.9094 - val_loss: 0.2525 - val_acc: 0.9088\n",
      "Epoch 3/200\n",
      "66414/66414 [==============================] - 3s 38us/step - loss: 0.2534 - acc: 0.9094 - val_loss: 0.2409 - val_acc: 0.9089\n",
      "Epoch 4/200\n",
      "66414/66414 [==============================] - 3s 38us/step - loss: 0.2446 - acc: 0.9094 - val_loss: 0.2393 - val_acc: 0.9097\n",
      "Epoch 5/200\n",
      "66414/66414 [==============================] - 3s 39us/step - loss: 0.2407 - acc: 0.9094 - val_loss: 0.2303 - val_acc: 0.9092\n",
      "Epoch 6/200\n",
      "66414/66414 [==============================] - 3s 39us/step - loss: 0.2361 - acc: 0.9089 - val_loss: 0.2394 - val_acc: 0.9099\n",
      "Epoch 7/200\n",
      "66414/66414 [==============================] - 3s 40us/step - loss: 0.2336 - acc: 0.9090 - val_loss: 0.2289 - val_acc: 0.9083\n",
      "Epoch 8/200\n",
      "66414/66414 [==============================] - 3s 38us/step - loss: 0.2321 - acc: 0.9095 - val_loss: 0.2269 - val_acc: 0.9076\n",
      "Epoch 9/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66414/66414 [==============================] - 3s 39us/step - loss: 0.2296 - acc: 0.9098 - val_loss: 0.2228 - val_acc: 0.9091\n",
      "Epoch 10/200\n",
      "66414/66414 [==============================] - 3s 39us/step - loss: 0.2276 - acc: 0.9099 - val_loss: 0.2246 - val_acc: 0.9088\n",
      "Epoch 11/200\n",
      "66414/66414 [==============================] - 3s 39us/step - loss: 0.2268 - acc: 0.9104 - val_loss: 0.2413 - val_acc: 0.9024\n",
      "Epoch 12/200\n",
      "66414/66414 [==============================] - 3s 38us/step - loss: 0.2266 - acc: 0.9105 - val_loss: 0.2209 - val_acc: 0.9089\n",
      "Epoch 13/200\n",
      "66414/66414 [==============================] - 3s 38us/step - loss: 0.2235 - acc: 0.9103 - val_loss: 0.2319 - val_acc: 0.9089\n",
      "Epoch 14/200\n",
      "66414/66414 [==============================] - 3s 38us/step - loss: 0.2223 - acc: 0.9105 - val_loss: 0.2247 - val_acc: 0.9078\n",
      "Epoch 15/200\n",
      "66414/66414 [==============================] - 3s 38us/step - loss: 0.2211 - acc: 0.9112 - val_loss: 0.2204 - val_acc: 0.9084\n",
      "Epoch 16/200\n",
      "66414/66414 [==============================] - 3s 38us/step - loss: 0.2197 - acc: 0.9113 - val_loss: 0.2342 - val_acc: 0.9095\n",
      "Epoch 17/200\n",
      "66414/66414 [==============================] - 3s 38us/step - loss: 0.2178 - acc: 0.9120 - val_loss: 0.2261 - val_acc: 0.9081: 0s - loss: 0.21\n",
      "Epoch 18/200\n",
      "66414/66414 [==============================] - 3s 38us/step - loss: 0.2161 - acc: 0.9121 - val_loss: 0.2267 - val_acc: 0.9089\n",
      "AUC: 0.871490\n",
      "Train on 66414 samples, validate on 7379 samples\n",
      "Epoch 1/200\n",
      "66414/66414 [==============================] - 4s 63us/step - loss: 0.3071 - acc: 0.9077 - val_loss: 0.2741 - val_acc: 0.9162\n",
      "Epoch 2/200\n",
      "66414/66414 [==============================] - 3s 39us/step - loss: 0.2688 - acc: 0.9086 - val_loss: 0.2480 - val_acc: 0.9162\n",
      "Epoch 3/200\n",
      "66414/66414 [==============================] - 3s 39us/step - loss: 0.2573 - acc: 0.9085 - val_loss: 0.2469 - val_acc: 0.9162\n",
      "Epoch 4/200\n",
      "66414/66414 [==============================] - 3s 38us/step - loss: 0.2495 - acc: 0.9086 - val_loss: 0.2421 - val_acc: 0.9162\n",
      "Epoch 5/200\n",
      "66414/66414 [==============================] - 3s 38us/step - loss: 0.2453 - acc: 0.9085 - val_loss: 0.2428 - val_acc: 0.9161\n",
      "Epoch 6/200\n",
      "66414/66414 [==============================] - 3s 38us/step - loss: 0.2396 - acc: 0.9085 - val_loss: 0.2309 - val_acc: 0.9160\n",
      "Epoch 7/200\n",
      "66414/66414 [==============================] - 3s 38us/step - loss: 0.2365 - acc: 0.9088 - val_loss: 0.2341 - val_acc: 0.9162\n",
      "Epoch 8/200\n",
      "66414/66414 [==============================] - 3s 39us/step - loss: 0.2323 - acc: 0.9092 - val_loss: 0.2295 - val_acc: 0.9152\n",
      "Epoch 9/200\n",
      "66414/66414 [==============================] - 3s 38us/step - loss: 0.2307 - acc: 0.9094 - val_loss: 0.2200 - val_acc: 0.9168\n",
      "Epoch 10/200\n",
      "66414/66414 [==============================] - 3s 38us/step - loss: 0.2290 - acc: 0.9088 - val_loss: 0.2276 - val_acc: 0.9149\n",
      "Epoch 11/200\n",
      "66414/66414 [==============================] - 3s 39us/step - loss: 0.2270 - acc: 0.9097 - val_loss: 0.2259 - val_acc: 0.9150\n",
      "Epoch 12/200\n",
      "66414/66414 [==============================] - 3s 39us/step - loss: 0.2261 - acc: 0.9088 - val_loss: 0.2239 - val_acc: 0.9154\n",
      "AUC: 0.853602\n",
      "[0.8619105926564532, 0.8615594005028636, 0.8634744311230731, 0.8675019060591265, 0.8726363264182692, 0.8503937677692103, 0.8669814841526106, 0.8672433212293409, 0.871489637586974, 0.8536015143007991]\n",
      "CV AUC: 0.863679\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 重新训练CNN word Embedding\n",
    "max_features = 29\n",
    "name = 'Embedding'\n",
    "gap = ''\n",
    "auc_mean=[]\n",
    "path_train = 'C:/Users/Crow/Desktop/human_data_12.12/Train.txt'\n",
    "path_test =  'C:/Users/Crow/Desktop/human_data_12.12/Independent.txt'\n",
    "\n",
    "x_train,y_train = pep(path_train,29-2)\n",
    "x_test,y_test = pep(path_test,29-2)\n",
    "\n",
    "kf = KFold(n_splits = 10,random_state=5,shuffle=True)\n",
    "j = 1 \n",
    "for train_index, test_index in kf.split(x_train):\n",
    "    x_train3, x_test3 = x_train[train_index], x_train[test_index]\n",
    "    y_train3, y_test3 = y_train[train_index], y_train[test_index]\n",
    "    \n",
    "#     model = create_cnn_model5(input_length=29)\n",
    "    \n",
    "#     early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "#     callbacks_list = [early_stopping]\n",
    "#     model.fit(x_train3, y_train3, validation_data = (x_test3, y_test3), epochs = 20, batch_size = 256,shuffle=True,\n",
    "#          callbacks=callbacks_list, verbose=1)\n",
    "    model = create_cnn_model6()\n",
    "    \n",
    "    #filepath='C:/Users/Crow/Desktop/new_result/CNN6/model/29_kfold_CNN_'+ name + gap+'_'+ str(j) +'.hdf5'\n",
    "   \n",
    "    #filepath=\"C:/Users/Crow/Desktop/result/re_CNN/model/weights.best.hdf5\"\n",
    "    #checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=False,mode='auto', period=50)\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "    callbacks_list = [early_stopping]\n",
    "    model.fit(x_train3, y_train3, validation_data = (x_test3, y_test3), epochs = 200, batch_size = 256,\n",
    "              shuffle=True,callbacks=callbacks_list, verbose=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    test_pred_proba = model.predict(x_test3)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test3,test_pred_proba,pos_label=1)\n",
    "    #print(\"ACC:  %f \"  %accuracy_score(y_test3,test_pred))\n",
    "    print(\"AUC: %f\" % auc(fpr, tpr))\n",
    "    auc_mean.append(auc(fpr, tpr))\n",
    "    #print(\"MCC: %f \" %matthews_corrcoef(y_test3,test_pred))\n",
    "#     fw = open('C:/Users/Crow/Desktop/new_result/CNN6/29_kfold_CNN_'+ name + gap+'_result_'+ str(j) +'.txt','w')\n",
    "#     for t in range(0,len(test_pred_proba)):\n",
    "#         fw.write(str(test_pred_proba[t][0]))\n",
    "#         fw.write('\\t')\n",
    "#         fw.write(str(y_test3[t]))\n",
    "#         fw.write('\\n')\n",
    "#     fw.close()\n",
    "    \n",
    "#     fw = open('C:/Users/Crow/Desktop/new_result/CNN6/29_kfold_CNN_'+ name + gap+'_test_'+ str(j) +'.txt','w')\n",
    "#     for t in range(0,len(y_test3)):\n",
    "#         fw.write(str(y_test3[t]))\n",
    "#         fw.write('\\n')\n",
    "#     fw.close()\n",
    "    \n",
    "    if j == 10:\n",
    "        print(auc_mean)\n",
    "        print(print(\"CV AUC: %f\" % mean(auc_mean)))\n",
    "#         model.save('C:/Users/Crow/Desktop/result/re_CNN/model/CNN_kfold_'+ name + gap +'.h5') \n",
    "        \n",
    "#         test_pred_proba = model.predict(x_test)\n",
    "#         fpr, tpr, thresholds = roc_curve(y_test,test_pred_proba,pos_label=1)\n",
    "#         print(\"总AUC: %f\" % auc(fpr, tpr))\n",
    "#         fw = open('C:/Users/Crow/Desktop/result/re_CNN/29_kfold_CNN_'+ name + gap +'_result.txt','w')\n",
    "#         for t in range(0,len(test_pred_proba)):\n",
    "#             fw.write(str(test_pred_proba[t][0]))\n",
    "#             fw.write('\\t')\n",
    "#             fw.write(str(y_test[t]))\n",
    "#             fw.write('\\n') \n",
    "#         fw.close()\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1000, 256)    77824       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 1000, 256)    0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 1000, 64)     49216       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1000, 64)     256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_1 (PReLU)               (None, 1000, 64)     64000       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1000, 64)     12352       p_re_lu_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 1000, 64)     256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 1000, 64)     16448       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_2 (PReLU)               (None, 1000, 64)     64000       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_3 (PReLU)               (None, 1000, 64)     64000       conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 1000, 64)     0           p_re_lu_2[0][0]                  \n",
      "                                                                 p_re_lu_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 499, 64)      0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 499, 64)      12352       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 499, 64)      256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_4 (PReLU)               (None, 499, 64)      31936       batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 499, 64)      12352       p_re_lu_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 499, 64)      256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_5 (PReLU)               (None, 499, 64)      31936       batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 499, 64)      0           p_re_lu_5[0][0]                  \n",
      "                                                                 max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 249, 64)      0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 249, 64)      12352       max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 249, 64)      256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_6 (PReLU)               (None, 249, 64)      15936       batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 249, 64)      12352       p_re_lu_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 249, 64)      256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_7 (PReLU)               (None, 249, 64)      15936       batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 249, 64)      0           p_re_lu_7[0][0]                  \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 124, 64)      0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 124, 64)      12352       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 124, 64)      256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_8 (PReLU)               (None, 124, 64)      7936        batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 124, 64)      12352       p_re_lu_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 124, 64)      256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_9 (PReLU)               (None, 124, 64)      7936        batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 124, 64)      0           p_re_lu_9[0][0]                  \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 61, 64)       0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 61, 64)       12352       max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 61, 64)       256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_10 (PReLU)              (None, 61, 64)       3904        batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 61, 64)       12352       p_re_lu_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 61, 64)       256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_11 (PReLU)              (None, 61, 64)       3904        batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 61, 64)       0           p_re_lu_11[0][0]                 \n",
      "                                                                 max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 30, 64)       0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 30, 64)       12352       max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 30, 64)       256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_12 (PReLU)              (None, 30, 64)       1920        batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 30, 64)       12352       p_re_lu_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 30, 64)       256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_13 (PReLU)              (None, 30, 64)       1920        batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 30, 64)       0           p_re_lu_13[0][0]                 \n",
      "                                                                 max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 14, 64)       0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 14, 64)       12352       max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 14, 64)       256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_14 (PReLU)              (None, 14, 64)       896         batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 14, 64)       12352       p_re_lu_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 14, 64)       256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_15 (PReLU)              (None, 14, 64)       896         batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 14, 64)       0           p_re_lu_15[0][0]                 \n",
      "                                                                 max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 64)           0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          16640       global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 256)          1024        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_16 (PReLU)              (None, 256)          256         batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           p_re_lu_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 6)            1542        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 644,166\n",
      "Trainable params: 564,038\n",
      "Non-trainable params: 80,128\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "max_features = 304\n",
    "embed_size = 256\n",
    "model = DPCNN1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303\n"
     ]
    }
   ],
   "source": [
    "# 构建词向量\n",
    "\n",
    "\n",
    "with open(\"security_test.csv.pkl\", \"rb\") as f:\n",
    "    file_names = pickle.load(f)\n",
    "    outfiles = pickle.load(f)\n",
    "with open(\"security_train.csv.pkl\", \"rb\") as f:\n",
    "    labels_d = pickle.load(f)\n",
    "with open(\"security_train.csv.pkl\", \"rb\") as f:\n",
    "    labels = pickle.load(f)\n",
    "    files = pickle.load(f)\n",
    "\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "labels = to_categorical(labels, num_classes=8)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=None,\n",
    "                      filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',\n",
    "                      split=' ',\n",
    "                      char_level=False,\n",
    "                      oov_token=None)\n",
    "tokenizer.fit_on_texts(files)\n",
    "tokenizer.fit_on_texts(outfiles)\n",
    "\n",
    "# with open(\"wordsdic.pkl\", 'wb') as f:\n",
    "#     pickle.dump(tokenizer, f)\n",
    "\n",
    "vocab = tokenizer.word_index\n",
    "#print(tokenizer.word_index)\n",
    "print(len(vocab))\n",
    "\n",
    "\n",
    "x_train_word_ids = tokenizer.texts_to_sequences(files)\n",
    "x_out_word_ids = tokenizer.texts_to_sequences(outfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 0\n",
      "2780 11107\n",
      "Train on 11107 samples, validate on 2780 samples\n",
      "Epoch 1/7\n",
      "11107/11107 [==============================] - 38s 3ms/step - loss: 1.4265 - acc: 0.4797 - val_loss: 1.1467 - val_acc: 0.5802\n",
      "Epoch 2/7\n",
      "11107/11107 [==============================] - 37s 3ms/step - loss: 0.9442 - acc: 0.6698 - val_loss: 0.8188 - val_acc: 0.7345\n",
      "Epoch 3/7\n",
      "11107/11107 [==============================] - 37s 3ms/step - loss: 0.7623 - acc: 0.7438 - val_loss: 0.7415 - val_acc: 0.7622\n",
      "Epoch 4/7\n",
      "11107/11107 [==============================] - 37s 3ms/step - loss: 0.6860 - acc: 0.7682 - val_loss: 0.6888 - val_acc: 0.7820\n",
      "Epoch 5/7\n",
      "11107/11107 [==============================] - 37s 3ms/step - loss: 0.6374 - acc: 0.7857 - val_loss: 0.6594 - val_acc: 0.7888\n",
      "Epoch 6/7\n",
      "11107/11107 [==============================] - 37s 3ms/step - loss: 0.6012 - acc: 0.7969 - val_loss: 0.6647 - val_acc: 0.7871\n",
      "Epoch 7/7\n",
      "11107/11107 [==============================] - 37s 3ms/step - loss: 0.5736 - acc: 0.8052 - val_loss: 0.6361 - val_acc: 0.8065\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Multioutput target data is not supported with label binarization",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-4b5ba57fc836>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;31m#model = load_model('model_weight.h5')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[0mpred_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[0mm_log_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm_log_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\deeplearning\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mlog_loss\u001b[1;34m(y_true, y_pred, eps, normalize, sample_weight, labels)\u001b[0m\n\u001b[0;32m   1769\u001b[0m         \u001b[0mlb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1770\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1771\u001b[1;33m         \u001b[0mlb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1772\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\deeplearning\\lib\\site-packages\\sklearn\\preprocessing\\label.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    405\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_type_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'multioutput'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_type_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m             raise ValueError(\"Multioutput target data is not supported with \"\n\u001b[0m\u001b[0;32m    408\u001b[0m                              \"label binarization\")\n\u001b[0;32m    409\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Multioutput target data is not supported with label binarization"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    DPCNN 3st: \n",
    "        filter_nr = 128 #滤波器通道个数\n",
    "        filter_size = 3 #卷积核\n",
    "        max_pool_size = 3 #池化层的pooling_size\n",
    "        max_pool_strides = 2 #池化层的步长\n",
    "        dense_nr = 256 #全连接层\n",
    "        spatial_dropout = 0.2\n",
    "        dense_dropout = 0.5\n",
    "        train_embed = False\n",
    "        conv_kern_reg = regularizers.l2(0.00001)\n",
    "        conv_bias_reg = regularizers.l2(0.00001)\n",
    "\"\"\"\n",
    "maxlen = 1000\n",
    "\n",
    "\n",
    "x_train_padded_seqs = pad_sequences(x_train_word_ids, maxlen=maxlen)\n",
    "x_out_padded_seqs = pad_sequences(x_out_word_ids, maxlen=maxlen)\n",
    "\n",
    "\n",
    "meta_train = np.zeros(shape=(len(x_train_padded_seqs), 8))\n",
    "meta_test = np.zeros(shape=(len(x_out_padded_seqs), 8))\n",
    "\n",
    "\n",
    "Fname = 'malware_attention_'\n",
    "Time = Fname + str(time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime()))\n",
    "tensorboard = TensorBoard(log_dir='./Logs/' + Time, histogram_freq=0, write_graph=True, write_images=False,\n",
    "                          embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, random_state=4, shuffle=True)\n",
    "for i, (tr_ind, te_ind) in enumerate(skf.split(x_train_padded_seqs, labels_d)):\n",
    "    print('FOLD: {}'.format(str(i)))\n",
    "    print(len(te_ind), len(tr_ind))\n",
    "    X_train, X_train_label = x_train_padded_seqs[tr_ind], labels[tr_ind]\n",
    "    X_val, X_val_label = x_train_padded_seqs[te_ind], labels[te_ind]\n",
    "\n",
    "    #model = dila()\n",
    "    \n",
    "    # 先测试TextCNN模型\n",
    "    #model = attention_model2()\n",
    "    model = attention_model2()\n",
    "\n",
    "    # model = load_model('model_weight.h5')\n",
    "    # print(model.summary())\n",
    "    # exit()\n",
    "#     model.compile(loss='categorical_crossentropy',\n",
    "#                   optimizer='adam',\n",
    "#                   metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "    model_save_path = 'model/model_weight_attention_st2_{}.h5'.format(str(i))\n",
    "    if i in [-1]:\n",
    "        model = model.load_weights(model_save_path)\n",
    "        print(model.evaluate(X_val, X_val_label))\n",
    "    else:\n",
    "        ear = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='min', baseline=None,\n",
    "                            restore_best_weights=False)\n",
    "\n",
    "        checkpoint = model_checkpoint = ModelCheckpoint(model_save_path, save_best_only=True, save_weights_only=True)\n",
    "        history = model.fit(X_train, X_train_label,\n",
    "                            batch_size=64,\n",
    "                            epochs=7,\n",
    "                            shuffle=True,\n",
    "                            validation_data=(X_val, X_val_label), callbacks=[tensorboard,checkpoint])\n",
    "        model.load_weights(model_save_path)\n",
    "        model.save('./model/model_weight_attention_st2_{}.h5'.format(str(i)))\n",
    "\n",
    "        #model = load_model('model_weight.h5')\n",
    "    pred_val = model.predict(X_val)\n",
    "    m_log_loss = log_loss(pred_val, X_val_label)\n",
    "    print(m_log_loss)\n",
    "    \n",
    "    \n",
    "    \n",
    "    pred_test = model.predict(x_out_padded_seqs)\n",
    "\n",
    "    meta_train[te_ind] = pred_val\n",
    "    meta_test += pred_test\n",
    "    K.clear_session()\n",
    "     \n",
    "meta_test /= 5.0\n",
    "with open(\"attention_st2_result.pkl\", 'wb') as f:\n",
    "    pickle.dump(meta_train, f)\n",
    "    pickle.dump(meta_test, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-68813f642c80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0minf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dpcnn_st3_result.pkl\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\deeplearning\\lib\\codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[1;31m# decode input (taking the buffer into account)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m         \u001b[1;31m# keep undecoded input until the next call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "inf = pickle.load(open(\"dpcnn_st3_result.pkl\",encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = []\n",
    "result = meta_test\n",
    "for i in range(len(file_names)):\n",
    "    tmp = []\n",
    "    a = result[i].tolist()\n",
    "\n",
    "    tmp.append(file_names[i])\n",
    "    tmp.extend(a)\n",
    "    out.append(tmp)\n",
    "with open(\"result_dpcnn_st3.csv\", \"w\", newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "\n",
    "    # 先写入columns_name\n",
    "    writer.writerow([\"file_id\", \"prob0\", \"prob1\", \"prob2\", \"prob3\", \"prob4\", \"prob5\", \"prob6\", \"prob7\"])\n",
    "    # 写入多行用writerows\n",
    "    writer.writerows(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13192 samples, validate on 695 samples\n",
      "Epoch 1/8\n",
      "13192/13192 [==============================] - 50s 4ms/step - loss: -21.6155 - acc: 0.2216 - val_loss: -32.6324 - val_acc: 0.1683\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "multiclass format is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-9626a68326ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLearningRateScheduler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mra_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRocAucEvaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mra_val\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\deeplearning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\deeplearning\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    215\u001b[0m                         \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m                             \u001b[0mepoch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\deeplearning\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m             \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-60-9626a68326ec>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterval\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\deeplearning\\lib\\site-packages\\sklearn\\metrics\\ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight, max_fpr)\u001b[0m\n\u001b[0;32m    354\u001b[0m     return _average_binary_score(\n\u001b[0;32m    355\u001b[0m         \u001b[0m_binary_roc_auc_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m         sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\deeplearning\\lib\\site-packages\\sklearn\\metrics\\base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[1;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[0my_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multilabel-indicator\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{0} format is not supported\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: multiclass format is not supported"
     ]
    }
   ],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n",
    "\n",
    "def schedule(ind):\n",
    "    a = [0.001, 0.0005, 0.0001, 0.0001]\n",
    "    return a[ind] \n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 8\n",
    "maxlen = 1000\n",
    "x_train_seq = pad_sequences(x_train_word_ids, maxlen=maxlen)\n",
    "\n",
    "Xtrain, Xval, ytrain, yval = train_test_split(x_train_seq, labels_d, train_size=0.95, random_state=233)\n",
    "\n",
    "lr = callbacks.LearningRateScheduler(schedule)\n",
    "ra_val = RocAucEvaluation(validation_data=(Xval, yval), interval = 1)\n",
    "model.fit(Xtrain, ytrain, batch_size=batch_size, epochs=epochs, validation_data=(Xval, yval), callbacks = [lr, ra_val] ,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

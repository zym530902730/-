{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "\n",
    "import math, os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "#from data.vocab import TextEncoder\n",
    "#from transformer.embedding import Embedding\n",
    "from keras.layers import *\n",
    "#from transformer.layers import MultiHeadAttention, Gelu, LayerNormalization\n",
    "\n",
    "# https://github.com/Separius/BERT-keras/blob/master/transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "import time\n",
    "from scipy import interp\n",
    "from sklearn import metrics\n",
    "from keras.models import load_model\n",
    "import csv\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#指定第一块GPU可用 \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "config = tf.ConfigProto() \n",
    "#不全部占满显存, 按需分配\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_list(x):\n",
    "    if K.backend() != 'theano':\n",
    "        tmp = K.int_shape(x)\n",
    "    else:\n",
    "        tmp = x.shape\n",
    "    tmp = list(tmp)\n",
    "    tmp[0] = -1\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def split_heads(x, n: int, k: bool = False):  # B, L, C\n",
    "    x_shape = shape_list(x)\n",
    "    m = x_shape[-1]\n",
    "    new_x_shape = x_shape[:-1] + [n, m // n]\n",
    "    new_x = K.reshape(x, new_x_shape)\n",
    "    return K.permute_dimensions(new_x, [0, 2, 3, 1] if k else [0, 2, 1, 3])\n",
    "\n",
    "\n",
    "def merge_heads(x):\n",
    "    new_x = K.permute_dimensions(x, [0, 2, 1, 3])\n",
    "    x_shape = shape_list(new_x)\n",
    "    new_x_shape = x_shape[:-2] + [np.prod(x_shape[-2:])]\n",
    "    return K.reshape(new_x, new_x_shape)\n",
    "\n",
    "\n",
    "# q and v are B, H, L, C//H ; k is B, H, C//H, L ; mask is B, 1, L, L\n",
    "def scaled_dot_product_attention_tf(q, k, v, attn_mask, attention_dropout: float, neg_inf: float):\n",
    "    w = K.batch_dot(q, k)  # w is B, H, L, L\n",
    "    w = w / K.sqrt(K.cast(shape_list(v)[-1], K.floatx()))\n",
    "    if attn_mask is not None:\n",
    "        w = attn_mask * w + (1.0 - attn_mask) * neg_inf\n",
    "    w = K.softmax(w)\n",
    "    w = Dropout(attention_dropout)(w)\n",
    "    return K.batch_dot(w, v)  # it is B, H, L, C//H [like v]\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention_th(q, k, v, attn_mask, attention_dropout: float, neg_inf: float):\n",
    "    w = theano_matmul(q, k)\n",
    "    w = w / K.sqrt(K.cast(shape_list(v)[-1], K.floatx()))\n",
    "    if attn_mask is not None:\n",
    "        attn_mask = K.repeat_elements(attn_mask, shape_list(v)[1], 1)\n",
    "        w = attn_mask * w + (1.0 - attn_mask) * neg_inf\n",
    "    w = K.T.exp(w - w.max()) / K.T.exp(w - w.max()).sum(axis=-1, keepdims=True)\n",
    "    w = Dropout(attention_dropout)(w)\n",
    "    return theano_matmul(w, v)\n",
    "\n",
    "\n",
    "def multihead_attention(x, attn_mask, n_head: int, n_state: int, attention_dropout: float, neg_inf: float):\n",
    "    _q, _k, _v = x[:, :, :n_state], x[:, :, n_state:2 * n_state], x[:, :, -n_state:]\n",
    "    q = split_heads(_q, n_head)  # B, H, L, C//H\n",
    "    k = split_heads(_k, n_head, k=True)  # B, H, C//H, L\n",
    "    v = split_heads(_v, n_head)  # B, H, L, C//H\n",
    "    if K.backend() == 'tensorflow':\n",
    "        a = scaled_dot_product_attention_tf(q, k, v, attn_mask, attention_dropout, neg_inf)\n",
    "    else:\n",
    "        a = scaled_dot_product_attention_th(q, k, v, attn_mask, attention_dropout, neg_inf)\n",
    "    return merge_heads(a)\n",
    "\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + K.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * K.pow(x, 3))))\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/a/42194662/2796084\n",
    "def theano_matmul(a, b, _left=False):\n",
    "    assert a.ndim == b.ndim\n",
    "    ndim = a.ndim\n",
    "    assert ndim >= 2\n",
    "    if _left:\n",
    "        b, a = a, b\n",
    "    if ndim == 2:\n",
    "        return K.T.dot(a, b)\n",
    "    else:\n",
    "        # If a is broadcastable but b is not.\n",
    "        if a.broadcastable[0] and not b.broadcastable[0]:\n",
    "            # Scan b, but hold a steady.\n",
    "            # Because b will be passed in as a, we need to left multiply to maintain\n",
    "            #  matrix orientation.\n",
    "            output, _ = K.theano.scan(theano_matmul, sequences=[b], non_sequences=[a[0], 1])\n",
    "        # If b is broadcastable but a is not.\n",
    "        elif b.broadcastable[0] and not a.broadcastable[0]:\n",
    "            # Scan a, but hold b steady.\n",
    "            output, _ = K.theano.scan(theano_matmul, sequences=[a], non_sequences=[b[0]])\n",
    "        # If neither dimension is broadcastable or they both are.\n",
    "        else:\n",
    "            # Scan through the sequences, assuming the shape for this dimension is equal.\n",
    "            output, _ = K.theano.scan(theano_matmul, sequences=[a, b])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'List' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-850e838e1113>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mTextEncoder\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mPAD_OFFSET\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mMSK_OFFSET\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mBOS_OFFSET\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mDEL_OFFSET\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m  \u001b[1;31m# delimiter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-850e838e1113>\u001b[0m in \u001b[0;36mTextEncoder\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'List' is not defined"
     ]
    }
   ],
   "source": [
    "class TextEncoder:\n",
    "    PAD_OFFSET = 0\n",
    "    MSK_OFFSET = 1\n",
    "    BOS_OFFSET = 2\n",
    "    DEL_OFFSET = 3  # delimiter\n",
    "    EOS_OFFSET = 4\n",
    "    SPECIAL_COUNT = 5\n",
    "    NUM_SEGMENTS = 2\n",
    "    BERT_UNUSED_COUNT = 99  # bert pretrained models\n",
    "    BERT_SPECIAL_COUNT = 4  # they don't have DEL\n",
    "\n",
    "    def __init__(self, vocab_size: int):\n",
    "        # NOTE you MUST always put unk at 0, then regular vocab, then special tokens, and then pos\n",
    "        self.vocab_size = vocab_size\n",
    "        self.unk_id = 0\n",
    "        self.pad_id = vocab_size + self.PAD_OFFSET\n",
    "        self.msk_id = vocab_size + self.MSK_OFFSET\n",
    "        self.bos_id = vocab_size + self.BOS_OFFSET\n",
    "        self.del_id = vocab_size + self.DEL_OFFSET\n",
    "        self.eos_id = vocab_size + self.EOS_OFFSET\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.vocab_size\n",
    "\n",
    "    def encode(self, sent: str) -> List[int]:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class SentencePieceTextEncoder(TextEncoder):\n",
    "    def __init__(self, text_corpus_address: Optional[str], model_name: str = 'spm',\n",
    "                 vocab_size: int = 30000, spm_model_type: str = 'unigram') -> None:\n",
    "        super().__init__(vocab_size)\n",
    "        if not os.path.exists('{}.model'.format(model_name)):\n",
    "            if spm_model_type.lower() not in ('unigram', 'bpe', 'char', 'word'):\n",
    "                raise ValueError(\n",
    "                    '{} is not a valid model_type for sentence piece, '\n",
    "                    'valid options are: unigram, bpe, char, word'.format(spm_model_type))\n",
    "            spm.SentencePieceTrainer.Train(\n",
    "                '--input={input} --model_prefix={model_name} --vocab_size={vocab_size} '\n",
    "                '--character_coverage={coverage} --model_type={model_type} '\n",
    "                '--pad_id=-1 --unk_id=0 --bos_id=-1 --eos_id=-1 --input_sentence_size=100000000 '.format(\n",
    "                    input=text_corpus_address, model_name=model_name, vocab_size=vocab_size, coverage=1,\n",
    "                    model_type=spm_model_type.lower()))\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.load('{}.model'.format(model_name))\n",
    "\n",
    "    def encode(self, sent: str) -> List[int]:\n",
    "        return self.sp.encode_as_ids(sent)\n",
    "\n",
    "\n",
    "class OpenAITextEncoder(TextEncoder):\n",
    "    def __init__(self, encoder_path: str = './openai/model/encoder_bpe_40000.json',\n",
    "                 bpe_path: str = './openai/model/vocab_40000.bpe') -> None:\n",
    "        self.encoder = _OpenAITextEncoder(encoder_path, bpe_path)\n",
    "        super().__init__(len(self.encoder.encoder))\n",
    "\n",
    "    def encode(self, sent: str) -> List[int]:\n",
    "        return self.encoder.encode([sent], verbose=False)[0]\n",
    "\n",
    "\n",
    "class BERTTextEncoder(TextEncoder):\n",
    "    def __init__(self, vocab_file: str, do_lower_case: bool = True) -> None:\n",
    "        self.tokenizer = FullTokenizer(vocab_file, do_lower_case)\n",
    "        super().__init__(len(self.tokenizer.vocab))\n",
    "        self.bert_unk_id = self.tokenizer.vocab['[UNK]']\n",
    "        self.bert_msk_id = self.tokenizer.vocab['[MASK]']\n",
    "\n",
    "    def standardize_ids(self, ids: List[int]) -> List[int]:\n",
    "        for i in range(len(ids)):\n",
    "            if ids[i] == self.bert_unk_id:  # UNK\n",
    "                ids[i] = 0\n",
    "            else:  # VOCAB\n",
    "                ids[i] -= self.bert_msk_id\n",
    "        return ids\n",
    "\n",
    "    def encode(self, sent: str) -> List[int]:\n",
    "        return self.standardize_ids(self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_pos_encoding_matrix(max_len: int, d_emb: int) -> np.array:\n",
    "    pos_enc = np.array(\n",
    "        [[pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)] if pos != 0 else np.zeros(d_emb) for pos in\n",
    "         range(max_len)], dtype=np.float32)\n",
    "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\n",
    "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\n",
    "    return pos_enc\n",
    "\n",
    "\n",
    "# NOTE that for vocab_size you should also add special_count\n",
    "class Embedding(keras.layers.Layer):\n",
    "    def __init__(self, output_dim: int = 768, dropout: float = 0.1, vocab_size: int = 30000 + TextEncoder.SPECIAL_COUNT,\n",
    "                 max_len: int = 512, trainable_pos_embedding: bool = True, use_one_dropout: bool = False,\n",
    "                 use_embedding_layer_norm: bool = False, layer_norm_epsilon: float = 1e-5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_len = max_len\n",
    "        self.use_one_dropout = use_one_dropout\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "        self.vocab_size = vocab_size\n",
    "        self.trainable_pos_embedding = trainable_pos_embedding\n",
    "\n",
    "        self.segment_emb = keras.layers.Embedding(TextEncoder.NUM_SEGMENTS, output_dim, input_length=max_len,\n",
    "                                                  name='SegmentEmbedding')\n",
    "        if not trainable_pos_embedding:\n",
    "            self.pos_emb = keras.layers.Embedding(max_len, output_dim, trainable=False, input_length=max_len,\n",
    "                                                  name='PositionEmbedding',\n",
    "                                                  weights=[_get_pos_encoding_matrix(max_len, output_dim)])\n",
    "        else:\n",
    "            self.pos_emb = keras.layers.Embedding(max_len, output_dim, input_length=max_len, name='PositionEmbedding')\n",
    "        self.token_emb = keras.layers.Embedding(vocab_size, output_dim, input_length=max_len, name='TokenEmbedding')\n",
    "        self.embedding_dropout = keras.layers.Dropout(dropout, name='EmbeddingDropOut')\n",
    "        self.add_embeddings = keras.layers.Add(name='AddEmbeddings')\n",
    "        self.use_embedding_layer_norm = use_embedding_layer_norm\n",
    "        if self.use_embedding_layer_norm:\n",
    "            self.embedding_layer_norm = LayerNormalization(layer_norm_epsilon)\n",
    "        else:\n",
    "            self.embedding_layer_norm = None\n",
    "        self.layer_norm_epsilon = layer_norm_epsilon\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0][0], input_shape[0][1], self.output_dim\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'max_len': self.max_len,\n",
    "            'use_one_dropout': self.use_one_dropout,\n",
    "            'output_dim': self.output_dim,\n",
    "            'dropout': self.dropout,\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'trainable_pos_embedding': self.trainable_pos_embedding,\n",
    "            'embedding_layer_norm': self.use_embedding_layer_norm,\n",
    "            'layer_norm_epsilon': self.layer_norm_epsilon\n",
    "        }\n",
    "        base_config = super().get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def __call__(self, inputs, **kwargs):\n",
    "        tokens, segment_ids, pos_ids = inputs\n",
    "        segment_embedding = self.segment_emb(segment_ids)\n",
    "        pos_embedding = self.pos_emb(pos_ids)\n",
    "        token_embedding = self.token_emb(tokens)\n",
    "        if self.use_one_dropout:\n",
    "            summation = self.add_embeddings([segment_embedding, pos_embedding, token_embedding])\n",
    "            if self.embedding_layer_norm:\n",
    "                summation = self.embedding_layer_norm(summation)\n",
    "            return self.embedding_dropout(summation)\n",
    "        summation = self.add_embeddings(\n",
    "            [self.embedding_dropout(segment_embedding), self.embedding_dropout(pos_embedding),\n",
    "             self.embedding_dropout(token_embedding)])\n",
    "        if self.embedding_layer_norm:\n",
    "            summation = self.embedding_layer_norm(summation)\n",
    "        return summation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, n_head: int, n_state: int, attention_dropout: float, use_attn_mask: bool, neg_inf: float,\n",
    "                 **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_head = n_head\n",
    "        self.n_state = n_state\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.use_attn_mask = use_attn_mask\n",
    "        self.neg_inf = neg_inf\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        x = input_shape[0] if self.use_attn_mask else input_shape\n",
    "        return x[0], x[1], x[2] // 3\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = inputs[0] if self.use_attn_mask else inputs\n",
    "        attn_mask = inputs[1] if self.use_attn_mask else None\n",
    "        return multihead_attention(x, attn_mask, self.n_head, self.n_state, self.attention_dropout, self.neg_inf)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'n_head': self.n_head,\n",
    "            'n_state': self.n_state,\n",
    "            'attention_dropout': self.attention_dropout,\n",
    "            'use_attn_mask': self.use_attn_mask,\n",
    "            'neg_inf': self.neg_inf,\n",
    "        }\n",
    "        base_config = super().get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class LayerNormalization(Layer):\n",
    "    def __init__(self, eps: float = 1e-5, **kwargs) -> None:\n",
    "        self.eps = eps\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:], initializer=Ones(), trainable=True)\n",
    "        self.beta = self.add_weight(name='beta', shape=input_shape[-1:], initializer=Zeros(), trainable=True)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x, **kwargs):\n",
    "        u = K.mean(x, axis=-1, keepdims=True)\n",
    "        s = K.mean(K.square(x - u), axis=-1, keepdims=True)\n",
    "        z = (x - u) / K.sqrt(s + self.eps)\n",
    "        return self.gamma * z + self.beta\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'eps': self.eps,\n",
    "        }\n",
    "        base_config = super().get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class Gelu(Layer):\n",
    "    def __init__(self, accurate: bool = False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.accurate = accurate\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if not self.accurate:\n",
    "            return gelu(inputs)\n",
    "        if K.backend() == 'tensorflow':\n",
    "            erf = K.tf.erf\n",
    "        else:\n",
    "            erf = K.T.erf\n",
    "        return inputs * 0.5 * (1.0 + erf(inputs / math.sqrt(2.0)))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'accurate': self.accurate,\n",
    "        }\n",
    "        base_config = super().get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention:\n",
    "    def __init__(self, n_state: int, n_head: int, attention_dropout: float,\n",
    "                 use_attn_mask: bool, layer_id: int, neg_inf: float) -> None:\n",
    "        assert n_state % n_head == 0\n",
    "        self.c_attn = Conv1D(3 * n_state, 1, name='layer_{}/c_attn'.format(layer_id))\n",
    "        self.attn = MultiHeadAttention(n_head, n_state, attention_dropout, use_attn_mask,\n",
    "                                       neg_inf, name='layer_{}/self_attention'.format(layer_id))\n",
    "        self.c_attn_proj = Conv1D(n_state, 1, name='layer_{}/c_attn_proj'.format(layer_id))\n",
    "\n",
    "    def __call__(self, x, mask):\n",
    "        output = self.c_attn(x)\n",
    "        output = self.attn(output) if mask is None else self.attn([output, mask])\n",
    "        return self.c_attn_proj(output)\n",
    "\n",
    "\n",
    "class PositionWiseFF:\n",
    "    def __init__(self, n_state: int, d_hid: int, layer_id: int, accurate_gelu: bool) -> None:\n",
    "        self.c_fc = Conv1D(d_hid, 1, name='layer_{}/c_fc'.format(layer_id))\n",
    "        self.activation = Gelu(accurate=accurate_gelu, name='layer_{}/gelu'.format(layer_id))\n",
    "        self.c_ffn_proj = Conv1D(n_state, 1, name='layer_{}/c_ffn_proj'.format(layer_id))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        output = self.activation(self.c_fc(x))\n",
    "        return self.c_ffn_proj(output)\n",
    "\n",
    "\n",
    "class EncoderLayer:\n",
    "    def __init__(self, n_state: int, n_head: int, d_hid: int, residual_dropout: float, attention_dropout: float,\n",
    "                 use_attn_mask: bool, layer_id: int, neg_inf: float, ln_epsilon: float, accurate_gelu: bool) -> None:\n",
    "        self.attention = MultiHeadSelfAttention(n_state, n_head, attention_dropout, use_attn_mask, layer_id, neg_inf)\n",
    "        self.drop1 = Dropout(residual_dropout, name='layer_{}/ln_1_drop'.format(layer_id))\n",
    "        self.add1 = Add(name='layer_{}/ln_1_add'.format(layer_id))\n",
    "        self.ln1 = LayerNormalization(ln_epsilon, name='layer_{}/ln_1'.format(layer_id))\n",
    "        self.ffn = PositionWiseFF(n_state, d_hid, layer_id, accurate_gelu)\n",
    "        self.drop2 = Dropout(residual_dropout, name='layer_{}/ln_2_drop'.format(layer_id))\n",
    "        self.add2 = Add(name='layer_{}/ln_2_add'.format(layer_id))\n",
    "        self.ln2 = LayerNormalization(ln_epsilon, name='layer_{}/ln_2'.format(layer_id))\n",
    "\n",
    "    def __call__(self, x, mask):\n",
    "        a = self.attention(x, mask)\n",
    "        n = self.ln1(self.add1([x, self.drop1(a)]))\n",
    "        f = self.ffn(n)\n",
    "        return self.ln2(self.add2([n, self.drop2(f)]))\n",
    "\n",
    "\n",
    "def create_transformer(embedding_dim: int = 768, embedding_dropout: float = 0.1, vocab_size: int = 30000,\n",
    "                       max_len: int = 512, trainable_pos_embedding: bool = True, num_heads: int = 12,\n",
    "                       num_layers: int = 12, attention_dropout: float = 0.1, use_one_embedding_dropout: bool = False,\n",
    "                       d_hid: int = 768 * 4, residual_dropout: float = 0.1, use_attn_mask: bool = True,\n",
    "                       embedding_layer_norm: bool = False, neg_inf: float = -1e9, layer_norm_epsilon: float = 1e-5,\n",
    "                       accurate_gelu: bool = False) -> keras.Model:\n",
    "    vocab_size += TextEncoder.SPECIAL_COUNT\n",
    "    tokens = Input(batch_shape=(None, max_len), name='token_input', dtype='int32')\n",
    "    segment_ids = Input(batch_shape=(None, max_len), name='segment_input', dtype='int32')\n",
    "    pos_ids = Input(batch_shape=(None, max_len), name='position_input', dtype='int32')\n",
    "    attn_mask = Input(batch_shape=(None, 1, max_len, max_len), name='attention_mask_input',\n",
    "                      dtype=K.floatx()) if use_attn_mask else None\n",
    "    inputs = [tokens, segment_ids, pos_ids]\n",
    "    embedding_layer = Embedding(embedding_dim, embedding_dropout, vocab_size, max_len, trainable_pos_embedding,\n",
    "                                use_one_embedding_dropout, embedding_layer_norm, layer_norm_epsilon)\n",
    "    x = embedding_layer(inputs)\n",
    "    for i in range(num_layers):\n",
    "        x = EncoderLayer(embedding_dim, num_heads, d_hid, residual_dropout,\n",
    "                         attention_dropout, use_attn_mask, i, neg_inf, layer_norm_epsilon, accurate_gelu)(x, attn_mask)\n",
    "    if use_attn_mask:\n",
    "        inputs.append(attn_mask)\n",
    "    return keras.Model(inputs=inputs, outputs=[x], name='Transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "class TextEncoder:\n",
    "    PAD_OFFSET = 0\n",
    "    MSK_OFFSET = 1\n",
    "    BOS_OFFSET = 2\n",
    "    DEL_OFFSET = 3  # delimiter\n",
    "    EOS_OFFSET = 4\n",
    "    SPECIAL_COUNT = 5\n",
    "    NUM_SEGMENTS = 2\n",
    "    BERT_UNUSED_COUNT = 99  # bert pretrained models\n",
    "    BERT_SPECIAL_COUNT = 4  # they don't have DEL\n",
    "\n",
    "    def __init__(self, vocab_size: int):\n",
    "        # NOTE you MUST always put unk at 0, then regular vocab, then special tokens, and then pos\n",
    "        self.vocab_size = vocab_size\n",
    "        self.unk_id = 0\n",
    "        self.pad_id = vocab_size + self.PAD_OFFSET\n",
    "        self.msk_id = vocab_size + self.MSK_OFFSET\n",
    "        self.bos_id = vocab_size + self.BOS_OFFSET\n",
    "        self.del_id = vocab_size + self.DEL_OFFSET\n",
    "        self.eos_id = vocab_size + self.EOS_OFFSET\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.vocab_size\n",
    "\n",
    "    def encode(self, sent: str) -> List[int]:\n",
    "        raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ldrgetprocedureaddress': 1, 'ntclose': 2, 'regqueryvalueexw': 3, 'thread32next': 4, 'ntdelayexecution': 5, 'regopenkeyexw': 6, 'getsystemmetrics': 7, 'regclosekey': 8, 'getcursorpos': 9, 'ntreadfile': 10, 'ntallocatevirtualmemory': 11, 'getkeystate': 12, 'ntqueryvaluekey': 13, 'process32nextw': 14, 'ntwritefile': 15, 'ldrloaddll': 16, 'getforegroundwindow': 17, 'ntquerydirectoryfile': 18, 'getsystemtimeasfiletime': 19, 'ldrgetdllhandle': 20, 'ntquerykey': 21, 'cryptdecodeobjectex': 22, 'ntopenkey': 23, 'findfirstfileexw': 24, 'ntcreatefile': 25, 'loadstringw': 26, 'ntopenkeyex': 27, 'ntprotectvirtualmemory': 28, 'ntfreevirtualmemory': 29, 'loadresource': 30, 'regqueryvalueexa': 31, 'setfilepointer': 32, 'timegettime': 33, 'readprocessmemory': 34, 'regopenkeyexa': 35, 'findresourceexw': 36, 'getfileattributesw': 37, 'seterrormode': 38, 'ntmapviewofsection': 39, 'getfiletype': 40, 'loadstringa': 41, 'ntcreatesection': 42, 'ntunmapviewofsection': 43, 'regenumkeyexw': 44, 'ntopenfile': 45, 'cocreateinstance': 46, 'regenumvaluew': 47, 'writeconsolew': 48, 'ldrunloaddll': 49, 'regsetvalueexa': 50, 'ntqueryinformationfile': 51, 'ntdeviceiocontrolfile': 52, 'ntopenprocess': 53, 'ntqueryattributesfile': 54, 'getasynckeystate': 55, 'getsysteminfo': 56, 'writeconsolea': 57, 'exception': 58, 'regenumkeyw': 59, 'ntcreatemutant': 60, 'sizeofresource': 61, 'drawtextexa': 62, 'ntduplicateobject': 63, 'regenumkeyexa': 64, 'ntterminateprocess': 65, 'getsystemwindowsdirectoryw': 66, 'coinitializeex': 67, 'getfilesize': 68, 'createthread': 69, 'couninitialize': 70, 'ntreadvirtualmemory': 71, 'findresourcew': 72, 'findwindowexw': 73, 'findresourcea': 74, 'ntopenmutant': 75, 'drawtextexw': 76, 'ntopensection': 77, 'setfilepointerex': 78, 'regcreatekeyexw': 79, 'getfileattributesexw': 80, 'regqueryinfokeya': 81, 'crypthashdata': 82, 'writeprocessmemory': 83, 'socket': 84, 'closesocket': 85, 'ntquerysysteminformation': 86, 'getsystemdirectoryw': 87, 'ntenumeratevaluekey': 88, 'regqueryinfokeyw': 89, 'ntenumeratekey': 90, 'deviceiocontrol': 91, 'regsetvalueexw': 92, 'createdirectoryw': 93, 'regdeletevaluew': 94, 'setunhandledexceptionfilter': 95, 'setfileattributesw': 96, 'getvolumepathnamesforvolumenamew': 97, 'select': 98, 'shgetfolderpathw': 99, 'wsastartup': 100, 'createtoolhelp32snapshot': 101, 'createprocessinternalw': 102, 'ntsetinformationfile': 103, 'ntresumethread': 104, 'getnativesysteminfo': 105, 'regcreatekeyexa': 106, 'getfilesizeex': 107, 'internetopena': 108, 'setendoffile': 109, 'ntsetvaluekey': 110, 'searchpathw': 111, 'gettemppathw': 112, 'enumwindows': 113, 'getshortpathnamew': 114, 'connect': 115, 'process32firstw': 116, 'ntopendirectoryobject': 117, 'internetopenurla': 118, 'deletefilew': 119, 'setfiletime': 120, 'cryptdecrypt': 121, 'setsockopt': 122, 'setstdhandle': 123, 'getvolumenameforvolumemountpointw': 124, 'cryptcreatehash': 125, 'globalmemorystatusex': 126, 'getaddrinfo': 127, 'createactctxw': 128, 'findwindowa': 129, 'ntopenthread': 130, 'gethostbyname': 131, 'module32nextw': 132, 'lookupprivilegevaluew': 133, 'send': 134, 'ntcreatekey': 135, 'oleinitialize': 136, 'openscmanagera': 137, 'findwindowexa': 138, 'cogetclassobject': 139, 'regenumvaluea': 140, 'ntcreatethreadex': 141, 'getadaptersaddresses': 142, 'copyfilea': 143, 'internetsetoptiona': 144, 'openservicea': 145, 'findwindoww': 146, 'getfileinformationbyhandleex': 147, 'setwindowshookexa': 148, 'getkeyboardstate': 149, 'internetclosehandle': 150, 'getsystemdirectorya': 151, 'uuidcreate': 152, 'getfileversioninfosizew': 153, 'getfileversioninfow': 154, 'isdebuggerpresent': 155, 'sendnotifymessagew': 156, 'openservicew': 157, 'gettimezoneinformation': 158, 'openscmanagerw': 159, 'getvolumepathnamew': 160, 'ioctlsocket': 161, 'createremotethread': 162, 'outputdebugstringa': 163, 'shgetspecialfolderlocation': 164, 'thread32first': 165, 'shellexecuteexw': 166, 'lookupaccountsidw': 167, 'createservicea': 168, 'getusernamew': 169, 'startservicea': 170, 'internetconnecta': 171, 'unhookwindowshookex': 172, 'getcomputernamew': 173, 'getaddrinfow': 174, 'wsasocketw': 175, 'setfileinformationbyhandle': 176, 'coinitializesecurity': 177, 'getfileinformationbyhandle': 178, 'getbestinterfaceex': 179, 'globalmemorystatus': 180, 'bind': 181, 'internetqueryoptiona': 182, 'urldownloadtofilew': 183, 'sendto': 184, 'cocreateinstanceex': 185, 'getcomputernamea': 186, 'cryptacquirecontexta': 187, 'setwindowshookexw': 188, 'cryptencrypt': 189, 'cryptacquirecontextw': 190, 'getdiskfreespaceexw': 191, 'certopenstore': 192, 'regdeletekeya': 193, 'movefilewithprogressw': 194, 'wsasocketa': 195, 'cryptexportkey': 196, 'ntqueryfullattributesfile': 197, 'getusernamea': 198, 'iwbemservices': 199, 'getsystemwindowsdirectorya': 200, 'execquery': 201, 'getadaptersinfo': 202, 'regdeletevaluea': 203, 'rtladdvectoredexceptionhandler': 204, 'internetcrackurla': 205, 'certcontrolstore': 206, 'recv': 207, 'listen': 208, 'rtlremovevectoredexceptionhandler': 209, 'recvfrom': 210, 'ntsuspendthread': 211, 'module32firstw': 212, 'findresourceexa': 213, 'accept': 214, 'ntqueueapcthread': 215, 'httpsendrequesta': 216, 'getsockname': 217, 'httpopenrequesta': 218, 'wsarecv': 219, 'internetcrackurlw': 220, 'httpqueryinfoa': 221, 'removedirectoryw': 222, 'getusernameexw': 223, 'certcreatecertificatecontext': 224, 'removedirectorya': 225, 'getdiskfreespacew': 226, 'copyfilew': 227, 'internetopenw': 228, 'internetconnectw': 229, 'httpopenrequestw': 230, 'internetgetconnectedstate': 231, 'httpsendrequestw': 232, 'ntgetcontextthread': 233, 'ntdeletekey': 234, 'rtladdvectoredcontinuehandler': 235, 'cryptunprotectmemory': 236, 'controlservice': 237, 'shutdown': 238, 'ntquerymultiplevaluekey': 239, 'regdeletekeyw': 240, 'internetsetstatuscallback': 241, 'getfileversioninfoexw': 242, 'getfileversioninfosizeexw': 243, 'cryptprotectmemory': 244, 'getinterfaceinfo': 245, 'internetreadfile': 246, 'ntdeletevaluekey': 247, 'startservicew': 248, 'createservicew': 249, 'createjobobjectw': 250, 'netshareenum': 251, 'setinformationjobobject': 252, 'sendnotifymessagea': 253, 'internetopenurlw': 254, 'registerhotkey': 255, 'deleteurlcacheentrya': 256, 'findfirstfileexa': 257, 'ntsavekey': 258, 'system': 259, 'copyfileexw': 260, 'ntterminatethread': 261, 'ntsetcontextthread': 262, 'deleteservice': 263, 'deleteurlcacheentryw': 264, 'wsaconnect': 265, 'netgetjoininformation': 266, 'obtainuseragentstring': 267, 'cryptprotectdata': 268, 'assignprocesstojobobject': 269, 'certopensystemstorea': 270, 'enumservicesstatusw': 271, 'rtlcreateuserthread': 272, 'ntwritevirtualmemory': 273, 'wsarecvfrom': 274, 'netusergetinfo': 275, 'rtlcompressbuffer': 276, 'enumservicesstatusa': 277, 'execmethod': 278, 'dnsquery': 279, 'wsasend': 280, 'cryptunprotectdata': 281, 'a': 282, 'rtldecompressbuffer': 283, 'cryptgenkey': 284, 'getusernameexa': 285, 'messageboxtimeouta': 286, 'w': 287, 'createremotethreadex': 288, 'certopensystemstorew': 289, 'internetgetconnectedstateexa': 290, 'ntloadkey': 291, 'readcabinetstate': 292, 'netusergetlocalgroups': 293, 'exitwindowsex': 294, 'internetgetconnectedstateexw': 295, 'createdirectoryexw': 296, 'messageboxtimeoutw': 297, 'wsasendto': 298, 'ntunloaddriver': 299, 'encryptmessage': 300, 'ntdeletefile': 301, 'taskdialog': 302, 'ntcreateuserprocess': 303}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "Fname = 'malware_'\n",
    "Time = Fname + str(time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime()))\n",
    "tensorboard = TensorBoard(log_dir='./Logs/' + Time, histogram_freq=0, write_graph=True, write_images=False,\n",
    "                          embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "\n",
    "with open(\"security_test.csv.pkl\", \"rb\") as f:\n",
    "    file_names = pickle.load(f)\n",
    "    outfiles = pickle.load(f)\n",
    "with open(\"security_train.csv.pkl\", \"rb\") as f:\n",
    "    labels_d = pickle.load(f)\n",
    "with open(\"security_train.csv.pkl\", \"rb\") as f:\n",
    "    labels = pickle.load(f)\n",
    "    files = pickle.load(f)\n",
    "maxlen = 10000\n",
    "\n",
    "\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "\n",
    "labels = to_categorical(labels, num_classes=8)\n",
    "tokenizer = Tokenizer(num_words=None,\n",
    "                      filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',\n",
    "                      split=' ',\n",
    "                      char_level=False,\n",
    "                      oov_token=None)\n",
    "tokenizer.fit_on_texts(files)\n",
    "tokenizer.fit_on_texts(outfiles)\n",
    "\n",
    "# with open(\"wordsdic.pkl\", 'wb') as f:\n",
    "#     pickle.dump(tokenizer, f)\n",
    "\n",
    "vocab = tokenizer.word_index\n",
    "print(tokenizer.word_index)\n",
    "\n",
    "x_train_word_ids = tokenizer.texts_to_sequences(files)\n",
    "x_out_word_ids = tokenizer.texts_to_sequences(outfiles)\n",
    "\n",
    "x_train_padded_seqs = pad_sequences(x_train_word_ids, maxlen=maxlen)\n",
    "\n",
    "x_out_padded_seqs = pad_sequences(x_out_word_ids, maxlen=maxlen)\n",
    "\n",
    "max_features = 10000\n",
    "meta_train = np.zeros(shape=(len(x_train_padded_seqs), 8))\n",
    "meta_test = np.zeros(shape=(len(x_out_padded_seqs), 8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18028"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train_word_ids[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
